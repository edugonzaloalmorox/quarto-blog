[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Setting up an AWS environment on Mac",
    "section": "",
    "text": "If you want to create a MLOps project, it is likely that you will interact with services offered by a cloud provider. For doing that you will likely require to link your local machine to any of the instances offered by the cloud provider. In this post, we will see how to set up the environment for a Mac machine to work with an EC2 instance on AWS."
  },
  {
    "objectID": "posts/post-with-code/index.html#footnotes",
    "href": "posts/post-with-code/index.html#footnotes",
    "title": "Setting up an AWS environment on Mac",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo view hidden folders on mac press Command + Shift + Dot‚Ü©Ô∏é\nchmod Is a Linux command that sets the permissions for file‚Äôs access. 0400 enables the user to read but can‚Äôt write nor execute.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html",
    "href": "posts/2023_09_10_atom_files/index.html",
    "title": "Extract information from atom files",
    "section": "",
    "text": "Sometimes valuable information often comes in the form of Atom or RSS files, which are essentially plain text formatted in XML. These files serve as a means to distribute content as feeds, ensuring users have access to the latest updates. However, while these files excel at timely information delivery, they can be challenging to decipher and parse. In this blog, I will show how Python can be your key to effortlessly extracting essential insights from these data sources."
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#locate-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#locate-the-data",
    "title": "Extract information from atom files",
    "section": "Locate the data",
    "text": "Locate the data\nFirst, find the data. For this example I will use information regarding public contracts released by the Spanish Ministry for Economic Affairs. These data are published on a daily basis each month. The current month data increases as information for each day is included.\n\nUnderstanding atom files\nAs any other feeds, an atom file is composed of two types of information: metadata and a set of entries.\n\nMetadata provide information about the author of the document (i.e.¬†the Ministry) as well as the URI domain, the date when the information was updated, etc..\nThe entry, on the other hand, provides information about the contract such as the ID of the contract, the price of the contract, the date, etc‚Ä¶\n\n\n\n\nRead the data\nTo read the data we require atoma and feedparser in Python. These will load a &lt;class 'atoma.atom.AtomFeed'&gt; in the system that cab be manipulated. This class has different arguments but the most interesting and the one that contains the relevant information is the &lt;entry&gt;. In our example this class provides information about the different contracts (ID of the contract, price, when the contract was updated, etc‚Ä¶ ) involving public institutions in Spain.\n\n\nCode\nimport atoma\nimport feedparser\n\nfeed = atoma.parse_atom_file('data/licitacionesPerfilesContratanteCompleto3.atom')\n\nprint(type(feed))\n\n\n&lt;class 'atoma.atom.AtomFeed'&gt;\n\n\nThis class contains several entries that are associated with a contract. To understand how many entries are contained in the .atom file we can use the length to show the number of entries in the .atom\n\n\nCode\nprint(len(feed.entries))\n\n\n500"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unknown data Pleasures‚Ä¶",
    "section": "",
    "text": "Wrapped 2023 in Ultracycling\n\n\n\n\n\n\n\nData Wrangling\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nEdu Gonzalo-Almorox\n\n\n\n\n\n\n  \n\n\n\n\nHow to find a needle in a haystack using machine learning\n\n\n\n\n\n\n\nML\n\n\nGIS\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nEdu Gonzalo-Almorox\n\n\n\n\n\n\n  \n\n\n\n\nExtract information from atom files\n\n\n\n\n\n\n\natom\n\n\nrss\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nEdu Gonzalo-Almorox\n\n\n\n\n\n\n  \n\n\n\n\nSetting up an AWS environment on Mac\n\n\n\n\n\n\n\nAWS\n\n\nMLOps\n\n\nEC2\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nEdu Gonzalo-Almorox\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me‚Ä¶",
    "section": "",
    "text": "I am Edu üëã , welcome to my blog!\nI am an economist turned into data scientist with international and multicultural experience in analytics. Throughout my career in several industries, I have been passionate about deriving actionable insights from complex data to address business needs and drive meaningful impact.\nMy areas of expertise involve the application of Machine Learning and (quasi) experimental methods. This blog aims to share reflections about these that may help others in their data journey. In short, a place where I can share different types of digital notes.\nIn addition to munging with data, outside work I üö¥‚Äç‚ôÇÔ∏è, sometimes for a while, and üèÉ‚Äç‚ôÇÔ∏è in¬†weird places. I also read and enjoy every kind of good music.\nEnjoy your stay and if you have comments or suggestions please do not hesitate to send me an email to¬†eduardogonzaloalmorox@gmail.com¬†or reach me out on¬†Twitter."
  },
  {
    "objectID": "about.html#in-a-nutshell",
    "href": "about.html#in-a-nutshell",
    "title": "About me‚Ä¶",
    "section": "",
    "text": "I am Edu üëã , welcome to my blog!\nI am an economist turned into data scientist with international and multicultural experience in analytics. Throughout my career in several industries, I have been passionate about deriving actionable insights from complex data to address business needs and drive meaningful impact.\nMy areas of expertise involve the application of Machine Learning and (quasi) experimental methods. This blog aims to share reflections about these that may help others in their data journey. In short, a place where I can share different types of digital notes.\nIn addition to munging with data, outside work I üö¥‚Äç‚ôÇÔ∏è, sometimes for a while, and üèÉ‚Äç‚ôÇÔ∏è in¬†weird places. I also read and enjoy every kind of good music.\nEnjoy your stay and if you have comments or suggestions please do not hesitate to send me an email to¬†eduardogonzaloalmorox@gmail.com¬†or reach me out on¬†Twitter."
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#parse-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#parse-the-data",
    "title": "Extract information from atom files",
    "section": "Parse the data",
    "text": "Parse the data\nOnce we have read the data, the next step is to parse the data so we can obtain the information of the contracts in a more structured way. A way to do so consists of querying each element that compose the entry component of the feed. An example of the information provided can be obtained by accessing the summary.value component of the entry.\n\n\nCode\n    print(feed.entries[0].summary.value)\n\n\nId licitaci√≥n: 4270012027200; √ìrgano de Contrataci√≥n: Jefatura de la Secci√≥n Econ√≥mico-Administrativa 27 - Base A√©rea de Getafe; Importe: 50377.14 EUR; Estado: RES\n\n\n\nObtaining information for all entries\nIn the code above, the information obtained corresponds only to the first contract. Yet, as we saw before, there is information referred to 500 public contracts. To parse information for all the entries of the feed object, we can define a function that extracts all the meaningful information for each contract and parses it in a data frame. Then we can loop through all the entries in the file.\nThis part of the code extracts relevant information from the contract and stores it in a dictionary that is saved as a data frame. The last part splits the different elements of the contract\n\nid of the contract\ncontractor or institution that perceives the contract\nprice of the contract\nstatus of the contract\n\n\n\nCode\nimport pandas as pd\n\n\n\ndef get_values(feed, n):  \n    objeto = feed.entries[n].title.value\n    perfil_contratante = feed.entries[n].id_\n    date_update = feed.entries[n].updated\n    link = feed.entries[n].links[0].href\n    licitacion = feed.entries[n].summary.value\n    \n    # Check for missing values and replace them with empty strings\n    objeto = objeto if objeto else \"\"\n    perfil_contratante = perfil_contratante if perfil_contratante else \"\"\n    date_update = date_update if date_update else \"\"\n    link = link if link else \"\"\n    licitacion = licitacion if licitacion else \"\"\n    \n    \n\n    dict = {\n        'objeto': objeto,\n        'perfil_contratante': perfil_contratante,\n        'fecha_actualizacion': date_update,\n        'link': link,\n        'licitacion': licitacion\n    } \n    \n    dict_df = pd.DataFrame(dict,index=[0])\n    \n    return dict_df\n\n\ndef clean_licitacion(df):\n    \n    lic = df[['licitacion']]\n    # Split the 'licitacion' column into new columns: 'id', 'organo', 'importe', 'estado'\n    split_value = lic['licitacion'].str.split(';', expand=True)\n\n\n    lic = df\n    \n    df_new = pd.concat([df, split_value], axis=1)\n   \n    df_new['id_licitacion'] = df_new[0].str.replace('Id licitaci√≥n: ', '')\n    df_new['organo'] = df_new[1].str.replace('√ìrgano de Contrataci√≥n: ', '')\n    df_new['importe'] = df_new[2].str.replace('Importe: ', '')\n    df_new['importe'] = df_new['importe'].str.replace(' EUR', '')\n    df_new['estado'] = df_new[3].str.replace('Estado: ', '')\n  \n     \n    \n    df_new = df_new[['objeto', 'perfil_contratante', 'fecha_actualizacion', 'link', 'id_licitacion', 'organo', 'importe', 'estado']]\n\n    return(df_new)\n\n\n\n\nCode\nlst_df = []\nlst = []\nn_values = list(range(1, len(feed.entries)))\nfor n in n_values:\n            dict_df = get_values(feed, n)\n            lst.append(dict_df)\n\n\nresult_df = pd.concat(lst, ignore_index=True)\n\nfinal_df = clean_licitacion(result_df)\nlst_df.append(final_df)\n\n\n        # Create the master_df by concatenating all the DataFrames\nmaster_df = pd.concat(lst_df, ignore_index=True)\nmaster_df.head(3)\n\n\n\n\n\n\n\n\n\nobjeto\nperfil_contratante\nfecha_actualizacion\nlink\nid_licitacion\norgano\nimporte\nestado\n\n\n\n\n0\nSuministro de energ√≠a el√©ctrica en el Centro d...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:35:38.409000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0109\nDirecci√≥n General del Instituto de Cinematogr...\n59722.28\nRES\n\n\n1\nServicio de duplicado de copias de pel√≠culas, ...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:34:24.393000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0133\nDirecci√≥n General del Instituto de Cinematogr...\n64000\nRES\n\n\n2\nGas Licuado Propano 2013/2014\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:16:16.416000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n204152041512012100\nSecci√≥n de Asuntos Econ√≥micos de la Academia ...\n49586.78\nRES"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#showing-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#showing-the-data",
    "title": "Extract information from atom files",
    "section": "Showing the data",
    "text": "Showing the data\nOnce we have the information parse in a structured format, we can make some analysis and obtain some insights. For instance, we can check what the top institutions by the price of their contracts are.\n\n\nCode\nimport plotly.graph_objs as go\n\n\nmaster_df['importe'] = pd.to_numeric(master_df['importe'], errors='coerce')\ngrouped_data = master_df.groupby('organo')['importe'].sum().reset_index()\n\ngrouped_data = grouped_data.sort_values(by='importe', ascending=False)\ntop_contractors = grouped_data.head(10)\n\nfig = go.Figure(data=[go.Pie(labels=top_contractors['organo'], values=top_contractors['importe'],\n                             hole=0.5, pull=[0.1, 0])])\n\n\n\nfig.update_layout(\n    title=dict(text=\"Top 10 contractors in January 2013\", x = 0.45, y=0.9), \n    legend=dict(orientation=\"h\", y=-5)\n)\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#conclusion",
    "href": "posts/2023_09_10_atom_files/index.html#conclusion",
    "title": "Extract information from atom files",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial you have seen how to parse and represent information from atom files using Python. This analysis can be replicated to the case of rss files.\nI hope you find it useful. Let me know in the comments if you have any questions or need further clarification."
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#atom-files-101",
    "href": "posts/2023_09_10_atom_files/index.html#atom-files-101",
    "title": "Extract information from atom files",
    "section": "Atom files 101",
    "text": "Atom files 101\nFirst, let‚Äôs locate the data .\nIn this blog, we‚Äôll be exploring a valuable source of information: public contracts. In particular, we will use the open data regarding public contracts released by the Spanish Ministry for Economic Affairs. These datasets are regularly updated, with new information becoming available each day throughout the month.\n\nDeconstructing Atom files\nBefore diving into the dasta, it‚Äôs essential to understand the structure of atom files. These files consist of two primary components: metadata and a set of entries.\n\nMetadata provide information about document‚Äôs author (i.e.¬†the Ministry), the domain‚Äôs URI, the date of last update, and more\nOn the other hand, entries contain detailed information about individual contracts, including contract IDs, prices, dates, and much more. These entries are the core of our data exploration journey.\n\n\n\n\nDiving into the data\nTo read the data we require atoma and feedparser in Python. These tool enable us to load a &lt;class 'atoma.atom.AtomFeed'&gt; in the system that can be manipulated. This class has different arguments but the most interesting and the one that contains the relevant information is the &lt;entry&gt;. In our case, this class provides information about the different contracts (ID of the contract, price, when the contract was updated, etc‚Ä¶ ) involving public institutions in Spain.\n\n\nCode\nimport atoma\nimport feedparser\n\nfeed = atoma.parse_atom_file('data/licitacionesPerfilesContratanteCompleto3.atom')\n\nprint(type(feed))\n\n\n&lt;class 'atoma.atom.AtomFeed'&gt;\n\n\nThis class contains several entries that are associated with a contract. To understand how many entries are contained in the .atom file we can use the length attribute to show the number of entries nested in the .atom\n\n\nCode\nprint(len(feed.entries))\n\n\n500"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#parsing-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#parsing-the-data",
    "title": "Extract information from atom files",
    "section": "Parsing the data",
    "text": "Parsing the data\nOnce we have succesfully read the data, the next step involves parsing the data so we can obtain contract- related information in a more organised and structured manner. Our approach queries each individual element that compose the entry component of the feed. For example, you can get a glimpse of the main elements of the contract by accessing the summary.value component of the entry.\n\n\nCode\n    print(feed.entries[0].summary.value)\n\n\nId licitaci√≥n: 4270012027200; √ìrgano de Contrataci√≥n: Jefatura de la Secci√≥n Econ√≥mico-Administrativa 27 - Base A√©rea de Getafe; Importe: 50377.14 EUR; Estado: RES\n\n\n\nRetrieving Information for all entries\nIn the code snippet provided earlier, we observed how to retrieve information for a single contract. However, as our data source comprises details on a sample of 500 public contracts, we need a more efficient approach. To accomplish this, we can define a function that systematically extracts meaningful information for each contract and organizes it into a structured data frame. We can then iterate through all the entries within the file.\nThe first part of the code (get_values()) extracts relevant information from the contract and stores it in a dictionary that is saved as a data frame. The last part (clean_licitacion()) splits the different elements of the contract:\n\nid of the contract\ncontractor or institution that perceives the contract\nprice of the contract\nstatus of the contract\n\n\n\nCode\nimport pandas as pd\n\n\n\ndef get_values(feed, n):  \n    objeto = feed.entries[n].title.value\n    perfil_contratante = feed.entries[n].id_\n    date_update = feed.entries[n].updated\n    link = feed.entries[n].links[0].href\n    licitacion = feed.entries[n].summary.value\n    \n    # Check for missing values and replace them with empty strings\n    objeto = objeto if objeto else \"\"\n    perfil_contratante = perfil_contratante if perfil_contratante else \"\"\n    date_update = date_update if date_update else \"\"\n    link = link if link else \"\"\n    licitacion = licitacion if licitacion else \"\"\n    \n    \n\n    dict = {\n        'objeto': objeto,\n        'perfil_contratante': perfil_contratante,\n        'fecha_actualizacion': date_update,\n        'link': link,\n        'licitacion': licitacion\n    } \n    \n    dict_df = pd.DataFrame(dict,index=[0])\n    \n    return dict_df\n\n\ndef clean_licitacion(df):\n    \n    lic = df[['licitacion']]\n    # Split the 'licitacion' column into new columns: 'id', 'organo', 'importe', 'estado'\n    split_value = lic['licitacion'].str.split(';', expand=True)\n\n\n    lic = df\n    \n    df_new = pd.concat([df, split_value], axis=1)\n   \n    df_new['id_licitacion'] = df_new[0].str.replace('Id licitaci√≥n: ', '')\n    df_new['organo'] = df_new[1].str.replace('√ìrgano de Contrataci√≥n: ', '')\n    df_new['importe'] = df_new[2].str.replace('Importe: ', '')\n    df_new['importe'] = df_new['importe'].str.replace(' EUR', '')\n    df_new['estado'] = df_new[3].str.replace('Estado: ', '')\n  \n     \n    \n    df_new = df_new[['objeto', 'perfil_contratante', 'fecha_actualizacion', 'link', 'id_licitacion', 'organo', 'importe', 'estado']]\n\n    return(df_new)\n\n\nThis code snippet loops through all the entries in the atom file.\n\n\nCode\nlst_df = []\nlst = []\nn_values = list(range(1, len(feed.entries)))\nfor n in n_values:\n            dict_df = get_values(feed, n)\n            lst.append(dict_df)\n\n\nresult_df = pd.concat(lst, ignore_index=True)\n\nfinal_df = clean_licitacion(result_df)\nlst_df.append(final_df)\n\n\n        # Create the master_df by concatenating all the DataFrames\nmaster_df = pd.concat(lst_df, ignore_index=True)\nmaster_df.head(3)\n\n\n\n\n\n\n\n\n\nobjeto\nperfil_contratante\nfecha_actualizacion\nlink\nid_licitacion\norgano\nimporte\nestado\n\n\n\n\n0\nSuministro de energ√≠a el√©ctrica en el Centro d...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:35:38.409000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0109\nDirecci√≥n General del Instituto de Cinematogr...\n59722.28\nRES\n\n\n1\nServicio de duplicado de copias de pel√≠culas, ...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:34:24.393000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0133\nDirecci√≥n General del Instituto de Cinematogr...\n64000\nRES\n\n\n2\nGas Licuado Propano 2013/2014\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:16:16.416000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n204152041512012100\nSecci√≥n de Asuntos Econ√≥micos de la Academia ...\n49586.78\nRES"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#visualise-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#visualise-the-data",
    "title": "Extract information from atom files",
    "section": "Visualise the data",
    "text": "Visualise the data\nOnce we have parsed and structured the information, we can make some analysis and obtain some insights. For instance, we can explore what the top institutions by the value of their contracts are.\n\n\nCode\nimport plotly.graph_objs as go\n\n\nmaster_df['importe'] = pd.to_numeric(master_df['importe'], errors='coerce')\ngrouped_data = master_df.groupby('organo')['importe'].sum().reset_index()\n\ngrouped_data = grouped_data.sort_values(by='importe', ascending=False)\ntop_contractors = grouped_data.head(10)\n\nfig = go.Figure(data=[go.Pie(labels=top_contractors['organo'], values=top_contractors['importe'],\n                             hole=0.5, pull=[0.1, 0])])\n\n\n\nfig.update_layout(\n    title=dict(text=\"Top 10 contractors in January 2013\", x = 0.45, y=0.9), \n    legend=dict(orientation=\"h\", y=-5)\n)\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "posts/2023_09_13_extract_info_from_atom_files/index.html",
    "href": "posts/2023_09_13_extract_info_from_atom_files/index.html",
    "title": "Extract information from atom files",
    "section": "",
    "text": "Pawel Czerwinski via Unsplash\nSometimes valuable information often comes in the form of Atom or RSS files, which are essentially plain text formatted in XML. These files serve as a means to distribute content as feeds, ensuring users have access to the latest updates. However, while these files excel at timely information delivery, they can be challenging to decipher and parse. In this blog, I will show how Python can be your key to effortlessly extracting essential insights from these data sources."
  },
  {
    "objectID": "posts/2023_09_13_extract_info_from_atom_files/index.html#atom-files-101",
    "href": "posts/2023_09_13_extract_info_from_atom_files/index.html#atom-files-101",
    "title": "Extract information from atom files",
    "section": "Atom files 101",
    "text": "Atom files 101\nFirst, let‚Äôs locate the data.\nIn this blog, we‚Äôll be exploring a valuable source of information: public contracts. In particular, we will use the open data regarding public contracts released by the Spanish Ministry for Economic Affairs. These datasets are regularly updated, with new information becoming available each day throughout the month.\n\nDeconstructing Atom files\nBefore diving into the data, it‚Äôs essential to understand the structure of atom files. These files consist of two primary components: metadata and a set of entries.\n\nMetadata provide information about document‚Äôs author (i.e.¬†the Ministry), the domain‚Äôs URI, the date of last update, and more\nOn the other hand, entries contain detailed information about individual contracts, including contract IDs, prices, dates, and much more. These entries are the core of our data exploration journey.\n\n\n\n\nDiving into the data\nTo read the data we require atoma and feedparser in Python. These tool enable us to load a &lt;class 'atoma.atom.AtomFeed'&gt; in the system that can be manipulated. This class has different arguments but the most interesting and the one that contains the relevant information is the &lt;entry&gt;. In our case, this class provides information about the different contracts (ID of the contract, price, when the contract was updated, etc‚Ä¶ ) involving public institutions in Spain.\n\n\nCode\nimport atoma\nimport feedparser\n\nfeed = atoma.parse_atom_file('data/licitacionesPerfilesContratanteCompleto3.atom')\n\nprint(type(feed))\n\n\n&lt;class 'atoma.atom.AtomFeed'&gt;\n\n\nThis class contains several entries that are associated with a contract. To understand how many entries are contained in the .atom file we can use the length attribute to show the number of entries nested in the .atom\n\n\nCode\nprint(len(feed.entries))\n\n\n500"
  },
  {
    "objectID": "posts/2023_09_13_extract_info_from_atom_files/index.html#parsing-the-data",
    "href": "posts/2023_09_13_extract_info_from_atom_files/index.html#parsing-the-data",
    "title": "Extract information from atom files",
    "section": "Parsing the data",
    "text": "Parsing the data\nOnce we have successfully read the data, the next step involves parsing the data so we can obtain contract- related information in a more organised and structured manner. Our approach queries each individual element that compose the entry component of the feed. For example, you can get a glimpse of the main elements of the contract by accessing the summary.value component of the entry.\n\n\nCode\n    print(feed.entries[0].summary.value)\n\n\nId licitaci√≥n: 4270012027200; √ìrgano de Contrataci√≥n: Jefatura de la Secci√≥n Econ√≥mico-Administrativa 27 - Base A√©rea de Getafe; Importe: 50377.14 EUR; Estado: RES\n\n\n\nRetrieving Information for all entries\nIn the code snippet provided earlier, we observed how to retrieve information for a single contract. However, as our data source comprises details on a sample of 500 public contracts, we need a more efficient approach. To accomplish this, we can define a function that systematically extracts meaningful information for each contract and organizes it into a structured data frame. We can then iterate through all the entries within the file.\nThe first part of the code (get_values()) extracts relevant information from the contract and stores it in a dictionary that is saved as a data frame. The last part (clean_licitacion()) splits the different elements of the contract:\n\nid of the contract\ncontractor or institution that perceives the contract\nprice of the contract\nstatus of the contract\n\n\n\nCode\nimport pandas as pd\n\n\n\ndef get_values(feed, n):  \n    objeto = feed.entries[n].title.value\n    perfil_contratante = feed.entries[n].id_\n    date_update = feed.entries[n].updated\n    link = feed.entries[n].links[0].href\n    licitacion = feed.entries[n].summary.value\n    \n    # Check for missing values and replace them with empty strings\n    objeto = objeto if objeto else \"\"\n    perfil_contratante = perfil_contratante if perfil_contratante else \"\"\n    date_update = date_update if date_update else \"\"\n    link = link if link else \"\"\n    licitacion = licitacion if licitacion else \"\"\n    \n    \n\n    dict = {\n        'objeto': objeto,\n        'perfil_contratante': perfil_contratante,\n        'fecha_actualizacion': date_update,\n        'link': link,\n        'licitacion': licitacion\n    } \n    \n    dict_df = pd.DataFrame(dict,index=[0])\n    \n    return dict_df\n\n\ndef clean_licitacion(df):\n    \n    lic = df[['licitacion']]\n    # Split the 'licitacion' column into new columns: 'id', 'organo', 'importe', 'estado'\n    split_value = lic['licitacion'].str.split(';', expand=True)\n\n\n    lic = df\n    \n    df_new = pd.concat([df, split_value], axis=1)\n   \n    df_new['id_licitacion'] = df_new[0].str.replace('Id licitaci√≥n: ', '')\n    df_new['organo'] = df_new[1].str.replace('√ìrgano de Contrataci√≥n: ', '')\n    df_new['importe'] = df_new[2].str.replace('Importe: ', '')\n    df_new['importe'] = df_new['importe'].str.replace(' EUR', '')\n    df_new['estado'] = df_new[3].str.replace('Estado: ', '')\n  \n     \n    \n    df_new = df_new[['objeto', 'perfil_contratante', 'fecha_actualizacion', 'link', 'id_licitacion', 'organo', 'importe', 'estado']]\n\n    return(df_new)\n\n\nThis code snippet loops through all the entries in the atom file.\n\n\nCode\nlst_df = []\nlst = []\nn_values = list(range(1, len(feed.entries)))\nfor n in n_values:\n            dict_df = get_values(feed, n)\n            lst.append(dict_df)\n\n\nresult_df = pd.concat(lst, ignore_index=True)\n\nfinal_df = clean_licitacion(result_df)\nlst_df.append(final_df)\n\n\n        # Create the master_df by concatenating all the DataFrames\nmaster_df = pd.concat(lst_df, ignore_index=True)\nmaster_df.head(3)\n\n\n\n\n\n\n\n\n\nobjeto\nperfil_contratante\nfecha_actualizacion\nlink\nid_licitacion\norgano\nimporte\nestado\n\n\n\n\n0\nSuministro de energ√≠a el√©ctrica en el Centro d...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:35:38.409000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0109\nDirecci√≥n General del Instituto de Cinematogr...\n59722.28\nRES\n\n\n1\nServicio de duplicado de copias de pel√≠culas, ...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:34:24.393000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0133\nDirecci√≥n General del Instituto de Cinematogr...\n64000\nRES\n\n\n2\nGas Licuado Propano 2013/2014\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:16:16.416000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n204152041512012100\nSecci√≥n de Asuntos Econ√≥micos de la Academia ...\n49586.78\nRES"
  },
  {
    "objectID": "posts/2023_09_13_extract_info_from_atom_files/index.html#visualise-the-data",
    "href": "posts/2023_09_13_extract_info_from_atom_files/index.html#visualise-the-data",
    "title": "Extract information from atom files",
    "section": "Visualise the data",
    "text": "Visualise the data\nOnce we have parsed and structured the information, we can make some analysis and obtain some insights. For instance, we can explore what the top institutions by the value of their contracts are.\n\n\nCode\nimport plotly.graph_objs as go\n\n\nmaster_df['importe'] = pd.to_numeric(master_df['importe'], errors='coerce')\ngrouped_data = master_df.groupby('organo')['importe'].sum().reset_index()\n\ngrouped_data = grouped_data.sort_values(by='importe', ascending=False)\ntop_contractors = grouped_data.head(10)\n\nfig = go.Figure(data=[go.Pie(labels=top_contractors['organo'], values=top_contractors['importe'],\n                             hole=0.5, pull=[0.1, 0])])\n\n\n\nfig.update_layout(\n    title=dict(text=\"Top 10 contractors in January 2013\", x = 0.45, y=0.9), \n    legend=dict(orientation=\"h\", y=-5)\n)\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "posts/2023_09_13_extract_info_from_atom_files/index.html#conclusion",
    "href": "posts/2023_09_13_extract_info_from_atom_files/index.html#conclusion",
    "title": "Extract information from atom files",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial you have seen how to parse and represent information from atom files using Python. This analysis can be replicated to the case of rss files.\nI hope you find it useful. Let me know in the comments if you have any questions or need further clarification."
  },
  {
    "objectID": "posts/2023_09_28_deploy_lambda_with_docker/index.html",
    "href": "posts/2023_09_28_deploy_lambda_with_docker/index.html",
    "title": "Deploy lambda functions with docker",
    "section": "",
    "text": "Lambda functions are an essential part of any machine learning model. They allow to run dif"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "",
    "text": "Imagine you‚Äôre a detective trying to track down a suspect in London. You have a few leads, but nothing concrete. One says the suspect was seen near the Bank of England. Another says it is near the Thames River. And a third says it may be within 3160 meters of a particular satellite path.\nCan you use machine learning to find the suspect? Yes, you can. You can do so by estimating the most likely point within a grid that reflects all possible locations where the suspect can be located.\nIn this blog I will show how to approach this problem from a machine learning perspective"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#define-the-grid",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#define-the-grid",
    "title": "Use ML to locate data in a map",
    "section": "Define the grid",
    "text": "Define the grid\n\n\nCode\nimport numpy as np # for data manipulation \nfrom sklearn.ensemble import RandomForestRegressor   # for modelling \nfrom sklearn.multioutput import MultiOutputRegressor # for modelling \nfrom sklearn.metrics import mean_squared_error       # for evaluation\nfrom sklearn.model_selection import train_test_split # for modelling \n#from utils.funs import haversine, great_circle_distance, gaussian_prob, lognormal_prob, normal_prob # for distances and probabiliteis\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import folium # for viz\n\n\n# Define the set of coordinates ---------\nthames_coordinates = [\n    (51.489467, -0.236313), (51.468045, -0.216379),\n    (51.464141, -0.190458), (51.473257, -0.179515),\n    (51.480661, -0.173850), (51.484590, -0.148573),\n    (51.483601, -0.137501), (51.485793, -0.129604),\n    (51.494744, -0.122824), (51.508208, -0.118489),\n    (51.509330, -0.096431), (51.501904, -0.058365),\n    (51.508662, -0.043216), (51.506098, -0.030727),\n    (51.490202, -0.028796), (51.485098, -0.007725),\n    (51.490683, 0.000215), (51.502305, -0.005407),\n    (51.506552, 0.005536)\n]\n\n\nboe_coordinates = [51.514171,-0.088438]\n\nsat_path_coords = [(51.451000, -0.300000), (51.560000, 0.000000)]\n\n\n\n\n# Define the grid of points ---------------\nlon_min, lon_max = -0.1, -0.04\nlat_min, lat_max = 51.45, 51.55\n\nn_points = 500\nlons = np.linspace(lon_min, lon_max, n_points)\nlats = np.linspace(lat_min, lat_max, n_points)\nlon_grid, lat_grid = np.meshgrid(lons, lats)"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#how-to-locate-the-researcher",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#how-to-locate-the-researcher",
    "title": "Use ML to locate data in a map",
    "section": "How to locate the researcher",
    "text": "How to locate the researcher\nThe primary goal of the code is to determine the most likely point within a grid that reflects all possible locations where there researcher can be located. The boundaries are\n\nCoordinates of river Thames\nCoordinates of the Bank of England\nCoordinates of satellite path\n\nTo calculate the most likely point within the grid I proceed following the next steps.\n\nDefinition of the grid.\nCalculate the distance between each point of the grid and each source of information provided - i.e.¬†river Thames, Bank of England and the satellite path.\nCalculate the probability of users being located in each point of the grid according to each source of information (an thus each statistical distribution)\nCombine the probabilities.\nObtain the most likely location within the grid.\nPlot the point obtained in 5\n\n\nDefine the grid points\n\n\nCode\nimport numpy as np # for data manipulation \nfrom sklearn.ensemble import RandomForestRegressor   # for modelling \nfrom sklearn.multioutput import MultiOutputRegressor # for modelling \nfrom sklearn.metrics import mean_squared_error       # for evaluation\nfrom sklearn.model_selection import train_test_split # for modelling \nfrom utils.funs import haversine, great_circle_distance, gaussian_prob, lognormal_prob, normal_prob # for distances and probabiliteis\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import folium # for viz\n\n\n# Define the set of coordinates ---------\nthames_coordinates = [\n    (51.489467, -0.236313), (51.468045, -0.216379),\n    (51.464141, -0.190458), (51.473257, -0.179515),\n    (51.480661, -0.173850), (51.484590, -0.148573),\n    (51.483601, -0.137501), (51.485793, -0.129604),\n    (51.494744, -0.122824), (51.508208, -0.118489),\n    (51.509330, -0.096431), (51.501904, -0.058365),\n    (51.508662, -0.043216), (51.506098, -0.030727),\n    (51.490202, -0.028796), (51.485098, -0.007725),\n    (51.490683, 0.000215), (51.502305, -0.005407),\n    (51.506552, 0.005536)\n]\n\n\nboe_coordinates = [51.514171,-0.088438]\n\nsat_path_coords = [(51.451000, -0.300000), (51.560000, 0.000000)]\n\n\n\n\n# Define the grid of points ---------------\nlon_min, lon_max = -0.1, -0.04\nlat_min, lat_max = 51.45, 51.55\n\nn_points = 500\nlons = np.linspace(lon_min, lon_max, n_points)\nlats = np.linspace(lat_min, lat_max, n_points)\nlon_grid, lat_grid = np.meshgrid(lons, lats)\n\n\n\n\nCalculate the distances and proabilities of each point of the grid\nExplanatory notes on distances and probabilities:\n\nDistance: The calculation of the distances considers the haversine distance. This distance gives the shortest distance between two points in sphere.\nProbabilities: Probabilities are calculated according to the statistical distributions provided in the exercise.\n\n\n\nCode\n#Calculate the distances and probabilities to each source\n\n# THAMES -----------------------------------\n\n\ndistance_to_thames = np.zeros((n_points, n_points))\n\nfor i in range(len(lons)):\n    for j in range(len(lats)):\n        # Calculate the distance from each point on the grid to the Thames\n        # using the Haversine formula for spherical geometry\n        distance_to_thames[j, i] = haversine(lon_grid[j, i], lat_grid[j, i], thames_coordinates[0])\n\nprob_thames = gaussian_prob(distance_to_thames)\n\n# BANK OF ENGLAND  --------------------------------\n\ndistance_to_boe = np.sqrt((lon_grid - boe_coordinates[1])**2 + (lat_grid - boe_coordinates[0])**2)\nprob_boe = lognormal_prob(distance_to_boe) \n\n# SATELLITE ----------------------------------------\n\ndistance_to_satellite_path = great_circle_distance(lon_grid, lat_grid, sat_path_coords)\nprob_satellite = normal_prob(distance_to_satellite_path)\n\n\n\n\nCombine probabilities\nProbabilities are combined with a stacking ensemble method by which the 3 sets of probabilities are combined fitting a RandomForest() model. This model uses the three sets of probabilities as features as well the coordinates of the points that compose the grid as output. Since this type of output is based on two elements (latitude and longitude), the random forest requires a MultiOutputRegressor()\n\n\nCode\n# Combine the three sets of probabilities into a single input array\nX = np.stack([prob_thames.flatten(), prob_boe.flatten(), prob_satellite.flatten()], axis=1)\n\n# Create the target array of grid points\nY = np.stack([lon_grid.flatten(), lat_grid.flatten()], axis=1)\n\n# Split the data into training and testing sets\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Create a multi-output RandomForestClassifier\n\nclf = RandomForestRegressor(n_estimators=100, random_state=42)\nmulti_clf = MultiOutputRegressor(clf)\n\n\n\n# Fit the classifier to the training data\nmulti_clf.fit(X_train, Y_train)\n\n\n# Predict the grid points for the test data\nY_pred = multi_clf.predict(X_test)\nrmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n\n\ndisplay(f\"The most likely location is {Y_pred[1]} and a root mean squared error (RMSE) on test set: {rmse:.4f}\")\n\n\n'The most likely location is [-0.0699982 51.4999544] and a root mean squared error (RMSE) on test set: 0.0239'\n\n\nThe coordinates where researcher is more likely to be are [51.4999544, -0.0699982]. Since we are running a regression model the evaluation of our results is on the basis of the Mean Square Error (MSE). The model provides which renders a 0.0239\n\n\nVisualisation of results\nTo visualise the solution I used folium() where I reflect the solution of the likely location in red. The output (i.e.¬†visualisation) is provided as leaflet map. See Readme instructions for a correct visualisation."
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#how-to-locate-the-researcher-1",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#how-to-locate-the-researcher-1",
    "title": "Use ML to locate data in a map",
    "section": "How to locate the researcher?",
    "text": "How to locate the researcher?"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#locating-the-suspect---first-attempt",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#locating-the-suspect---first-attempt",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Locating the suspect - First attempt",
    "text": "Locating the suspect - First attempt\nIn this first attempt, we will use a grid search to locate the most likely point within a grid that reflects all possible locations where the suspect can be located. We can do this by using a numpy meshgrid. This function allows to create a two-dimensional grid of points, where each point is defined by a pair of coordinates. We can set the boundaries of the grid to be the coordinates of the River Thames, the Bank of England, and the satellite path. This will ensure that the grid covers all possible locations where the researcher could be.\n\n\nCode\nimport numpy as np # for data manipulation \nfrom sklearn.ensemble import RandomForestRegressor   # for modelling \nfrom sklearn.multioutput import MultiOutputRegressor # for modelling \nfrom sklearn.metrics import mean_squared_error       # for evaluation\nfrom sklearn.model_selection import train_test_split # for modelling \nimport folium # for viz\n\nimport numpy as np\nfrom scipy.stats import norm, lognorm\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\n\n#from utils.funs import haversine, great_circle_distance, gaussian_prob, lognormal_prob, normal_prob # for distances and probabilities \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n# Define the set of coordinates ---------\nthames_coordinates = [\n    (51.489467, -0.236313), (51.468045, -0.216379),\n    (51.464141, -0.190458), (51.473257, -0.179515),\n    (51.480661, -0.173850), (51.484590, -0.148573),\n    (51.483601, -0.137501), (51.485793, -0.129604),\n    (51.494744, -0.122824), (51.508208, -0.118489),\n    (51.509330, -0.096431), (51.501904, -0.058365),\n    (51.508662, -0.043216), (51.506098, -0.030727),\n    (51.490202, -0.028796), (51.485098, -0.007725),\n    (51.490683, 0.000215), (51.502305, -0.005407),\n    (51.506552, 0.005536)\n]\n\n\nboe_coordinates = [51.514171,-0.088438]\n\nsat_path_coords = [(51.451000, -0.300000), (51.560000, 0.000000)]\n\n\n\n\n# Define the grid of points ---------------\nlon_min, lon_max = -0.1, -0.04\nlat_min, lat_max = 51.45, 51.55\n\nn_points = 500\nlons = np.linspace(lon_min, lon_max, n_points)\nlats = np.linspace(lat_min, lat_max, n_points)\nlon_grid, lat_grid = np.meshgrid(lons, lats)"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#predicting-the-location-of-the-suspect",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#predicting-the-location-of-the-suspect",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Predicting the location of the suspect",
    "text": "Predicting the location of the suspect\nOnce we have defined the grid, we need to calculate the likelihood of the suspect being at each point. To do that, we first need to calculate the distance from each point to each source of information.\n\nCalculate the distances\nTo find the most likely distance between the suspect‚Äôs possible locations and the known landmarks, we can calculate the distance between them. There are two types of distances we can use: Haversine distance and Great Circle distance.\nThe Haversine distance represents the shortest distance between two points on a sphere. We can use the following function to calculate it:\n\n\nCode\ndef haversine(lon, lat, coords):\n    \n    '''\n    Returns the haversine distance between a pair of coordinates and the coordinates of the grid\n    \n     Parameters\n    ------------\n        lon: int, Longitude of a point in the grid\n        lat: int, Latitude of a point in the grid\n        coords: tuple, Coordinates of the piece of information\n            \n    Return\n    -----------\n        distance : int\n            Difference between pair of coordinates and the coordinates of the grid \n    ''' \n    \n    lon1, lat1 = coords\n    R = 6371  # Earth radius in km\n    diff_lat = np.radians(lat1 - lat)\n    diff_lon = np.radians(lon1 - lon)\n    a = np.sin(diff_lat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat)) * np.sin(diff_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    return distance\n\n\nThe Great Circle distance, on the other hand, is another type of distance we can use to calculate the distance between two points on a sphere. It‚Äôs the shortest distance along the great circle, which is the imaginary circle that encircles the sphere and passes through the two points.\nThis type of distance can be a good alternative in cases where there is not a clear trajectory between the points. In the case of the Thames which is a long, winding river, the shortest distance between two points on the river may not be the shortest distance between the two points on the grid.\n\n\nCode\ndef great_circle_distance(lon, lat, coords):\n    \n    '''\n    Returns the great circle distance between a pair of coordinates and the coordinates of the grid\n    \n     Parameters\n    ------------\n        lon: int, Longitude of a point in the grid\n        lat: int, Latitude of a point in the grid\n        coords: tuple, Coordinates of the piece of information\n            \n    Return\n    -----------\n        distance : int\n            Difference between pair of coordinates and the coordinates of the grid \n    '''\n    \n    lon1, lat1 = coords[0]\n    lon2, lat2 = coords[1]\n    R = 6371  # Earth radius in km\n    diff_lat = np.radians(lat2 - lat1)\n    diff_lon = np.radians(lon2 - lon1)\n    a = np.sin(diff_lat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(diff_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    # Calculate the distance between each point on the grid and the great circle path\n    distance_to_path = np.abs(distance * np.sin(np.radians(lon) - np.radians(coords[0][0])))\n    return distance_to_path\n\n\n\n\nCalculate the probabilities\nIn addition to the distances, we need to figure out what location is the most likely for suspect to be located. We can do this calculating the probabilities of the suspect to be at each possible location within the grid. We can use different statistical distributions to calculate the probability.\nA common approach is to use a normal distribution, also known as a Gaussian distribution. This distribution is bell-shaped, with the highest probability in the center and the probability decreasing as you move away from the center.\n\n\nCode\n# Probability functions \ndef gaussian_prob(distance_to_thames):\n    '''\n    Returns a gaussian probability\n    \n     Parameters\n    ------------\n        distance_to_thames: int, distance (from the grid point) to thames (point)\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to thames\n            \n    '''\n\n    pdf = norm.pdf(distance_to_thames, loc=0, scale=2730)\n    return pdf\n\n\n\ndef normal_prob(distance_to_satellite_path):\n    '''\n    Returns a lognormal probability\n    \n     Parameters\n    ------------\n        distance_to_satellite: int, distance (from the grid point) to points of the satellite path\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to the satellite points of the path\n            \n    '''\n    pdf = norm.pdf(distance_to_satellite_path, loc=0, scale=3160)\n    return pdf\n\n\nAlso, when we have data with unknown information, such as the coordinates of the Bank of England, we can use the Lognormal distribution to model it.\nThe Lognormal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. This means that if we take the logarithm of all the values in a Lognormal distribution, we will get a normal distribution. Lognormal distributions are often used to model data that is skewed to the right.\nBank of England coordinates are likely to be skewed to the right, with a few very high values and many lower values\n\n\nCode\ndef lognormal_prob(distance_to_boe):\n    '''\n    Returns a lognormal probability\n    \n     Parameters\n    ------------\n        distance_to_boe: int, distance (from the grid point) to Bank of England (point)\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to Bank of England\n            \n    '''\n    pdf = lognorm.pdf(distance_to_boe, s=0.625, scale=np.exp(8.460))\n    return pdf\n\n\nOnce we have the distances from the suspect to each point on the grid, we can use them to calculate the probability of the suspect being at each point. To calculate the probability, we use the statistical distributions defined above\n\n\nCode\n#Calculate distances and probabilities to each source\n\n# THAMES -----------------------------------\n\n\ndistance_to_thames = np.zeros((n_points, n_points))\n\nfor i in range(len(lons)):\n    for j in range(len(lats)):\n        # Calculate the distance from each point on the grid to the Thames\n        # using the Haversine formula for spherical geometry\n        distance_to_thames[j, i] = haversine(lon_grid[j, i], lat_grid[j, i], thames_coordinates[0])\n\nprob_thames = gaussian_prob(distance_to_thames)\n\n# BANK OF ENGLAND  --------------------------------\n\ndistance_to_boe = np.sqrt((lon_grid - boe_coordinates[1])**2 + (lat_grid - boe_coordinates[0])**2)\nprob_boe = lognormal_prob(distance_to_boe) \n\n# SATELLITE ----------------------------------------\n\ndistance_to_satellite_path = great_circle_distance(lon_grid, lat_grid, sat_path_coords)\nprob_satellite = normal_prob(distance_to_satellite_path)\n\n\n\n\nCombine the probabilities for each point\nOnce we have calculated the probability of the suspect being at each point on the grid, we can combine these probabilities to get a single probability distribution for the suspect‚Äôs location.\nOne way to combine probabilities is to use a stacking ensemble method. Stacking ensemble methods combine the predictions of multiple machine learning models to produce a more accurate prediction.\nIn our case, we can do this by training a Machine Learning method, RandomForest(), on the probabilities from the three sources of information to estimate the (most likely) coordinates that compose the grid. Since this type of output is based on two elements (latitude and longitude), the random forest requires a MultiOutputRegressor()\n\n\nCode\n# Combine the three sets of probabilities into a single input array\nX = np.stack([prob_thames.flatten(), prob_boe.flatten(), prob_satellite.flatten()], axis=1)\n\n# Create the target array of grid points\nY = np.stack([lon_grid.flatten(), lat_grid.flatten()], axis=1)\n\n# Split the data into training and testing sets\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Create a multi-output RandomForestClassifier\n\nclf = RandomForestRegressor(n_estimators=100, random_state=42)\nmulti_clf = MultiOutputRegressor(clf)\n\n\n\n# Fit the classifier to the training data\nmulti_clf.fit(X_train, Y_train)\n\n\n# Predict the grid points for the test data\nY_pred = multi_clf.predict(X_test)\nrmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n\n\ndisplay(f\"The most likely location is {Y_pred[1]} and a root mean squared error (RMSE) on test set: {rmse:.4f}\")\n\n\n'The most likely location is [-0.0699982 51.4999544] and a root mean squared error (RMSE) on test set: 0.0239'\n\n\nThe coordinates where researcher is more likely to be are [51.4999544, -0.0699982]. Since we are running a regression model the evaluation of our results is on the basis of the Mean Square Error (MSE). The model provides which renders a 0.0239"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#visualisation-of-results",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#visualisation-of-results",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Visualisation of results",
    "text": "Visualisation of results\nTo visualise the solution I used folium() where I reflect the solution of the likely location in red. The output (i.e.¬†visualisation) is provided as leaflet map. See Readme instructions for a correct visualisation.\n\n\nCode\nlondon_coords = [51.509865, -0.118092]\nresearcher_coords = [51.4999544,-0.0699982]\nboe_coords = [51.514171,-0.088438]\n\nlondon_map = folium.Map(location=london_coords, zoom_start=12.5)\n\n\n\nfolium.Marker(\n    location= researcher_coords,\n    popup=\"Researcher likely location\",\n    icon=folium.Icon(color=\"red\"),\n).add_to(london_map)\n\nfolium.Circle(\n    radius=500,\n    location= researcher_coords,\n    popup=\"Researcher likely location\",\n    fill=False,\n).add_to(london_map)\n\n\nfolium.Marker(\n    location= boe_coords,\n    popup=\"Bank of England\",\n    icon=folium.Icon(color=\"green\", icon=\"building-columns\"),\n).add_to(london_map)\n\n\n\nlondon_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#locating-the-suspect",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#locating-the-suspect",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Locating the suspect",
    "text": "Locating the suspect\nIn a first attempt, we will use a grid search to locate the most likely point within a grid that reflects all possible locations where the suspect can be located. We can do this by using a numpy meshgrid. This function allows to create a two-dimensional grid of points, where each point is defined by a pair of coordinates. We can set the boundaries of the grid to be the coordinates of the River Thames, the Bank of England, and the satellite path. This will ensure that the grid covers all possible locations where the researcher could be.\n\n\nCode\nimport numpy as np # for data manipulation \nfrom sklearn.ensemble import RandomForestRegressor   # for modelling \nfrom sklearn.multioutput import MultiOutputRegressor # for modelling \nfrom sklearn.metrics import mean_squared_error       # for evaluation\nfrom sklearn.model_selection import train_test_split # for modelling \nimport folium # for viz\n# Packages for functions -----------------------------------\nimport numpy as np\nfrom scipy.stats import norm, lognorm\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n# Define the set of coordinates ---------\nthames_coordinates = [\n    (51.489467, -0.236313), (51.468045, -0.216379),\n    (51.464141, -0.190458), (51.473257, -0.179515),\n    (51.480661, -0.173850), (51.484590, -0.148573),\n    (51.483601, -0.137501), (51.485793, -0.129604),\n    (51.494744, -0.122824), (51.508208, -0.118489),\n    (51.509330, -0.096431), (51.501904, -0.058365),\n    (51.508662, -0.043216), (51.506098, -0.030727),\n    (51.490202, -0.028796), (51.485098, -0.007725),\n    (51.490683, 0.000215), (51.502305, -0.005407),\n    (51.506552, 0.005536)\n]\n\n\nboe_coordinates = [51.514171,-0.088438]\n\nsat_path_coords = [(51.451000, -0.300000), (51.560000, 0.000000)]\n\n\n\n\n# Define the grid of points ---------------\nlon_min, lon_max = -0.1, -0.04\nlat_min, lat_max = 51.45, 51.55\n\nn_points = 500\nlons = np.linspace(lon_min, lon_max, n_points)\nlats = np.linspace(lat_min, lat_max, n_points)\nlon_grid, lat_grid = np.meshgrid(lons, lats)"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#visualising-the-location-of-the-suspect",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#visualising-the-location-of-the-suspect",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Visualising the location of the suspect",
    "text": "Visualising the location of the suspect\nTo visualise the solution I used folium() where I reflect the solution of the likely location in red. The output (i.e.¬†visualisation) is provided as leaflet map. See Readme instructions for a correct visualisation.\n\n\nCode\nlondon_coords = [51.509865, -0.118092]\nresearcher_coords = [51.4999544,-0.0699982]\nboe_coords = [51.514171,-0.088438]\n\nlondon_map = folium.Map(location=london_coords, zoom_start=12.5)\n\n\n\nfolium.Marker(\n    location= researcher_coords,\n    popup=\"Researcher likely location\",\n    icon=folium.Icon(color=\"red\"),\n).add_to(london_map)\n\nfolium.Circle(\n    radius=500,\n    location= researcher_coords,\n    popup=\"Researcher likely location\",\n    fill=False,\n).add_to(london_map)\n\n\nfolium.Marker(\n    location= boe_coords,\n    popup=\"Bank of England\",\n    icon=folium.Icon(color=\"green\", icon=\"building-columns\"),\n).add_to(london_map)\n\n\n\nlondon_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#defining-a-grid",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#defining-a-grid",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Defining a grid",
    "text": "Defining a grid\nFirst, we need to define a grid that covers all possible locations where the suspect could be. We can do this using a meshgrid() function in NumPy. This function creates a two-dimensional grid of points, where each point is defined by a pair of coordinates.\nWe can set the boundaries of the grid to be the coordinates of the River Thames, the Bank of England, and the satellite path. This will ensure that the grid covers all possible locations where the suspect could be.\n\n\nCode\nimport numpy as np # for data manipulation \nfrom sklearn.ensemble import RandomForestRegressor   # for modelling \nfrom sklearn.multioutput import MultiOutputRegressor # for modelling \nfrom sklearn.metrics import mean_squared_error       # for evaluation\nfrom sklearn.model_selection import train_test_split # for modelling \nimport folium # for viz\n# Packages for functions -----------------------------------\nimport numpy as np\nfrom scipy.stats import norm, lognorm\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n# Define the set of coordinates ---------\nthames_coordinates = [\n    (51.489467, -0.236313), (51.468045, -0.216379),\n    (51.464141, -0.190458), (51.473257, -0.179515),\n    (51.480661, -0.173850), (51.484590, -0.148573),\n    (51.483601, -0.137501), (51.485793, -0.129604),\n    (51.494744, -0.122824), (51.508208, -0.118489),\n    (51.509330, -0.096431), (51.501904, -0.058365),\n    (51.508662, -0.043216), (51.506098, -0.030727),\n    (51.490202, -0.028796), (51.485098, -0.007725),\n    (51.490683, 0.000215), (51.502305, -0.005407),\n    (51.506552, 0.005536)\n]\n\n\nboe_coordinates = [51.514171,-0.088438]\n\nsat_path_coords = [(51.451000, -0.300000), (51.560000, 0.000000)]\n\n\n\n\n# Define the grid of points ---------------\nlon_min, lon_max = -0.1, -0.04\nlat_min, lat_max = 51.45, 51.55\n\nn_points = 500\nlons = np.linspace(lon_min, lon_max, n_points)\nlats = np.linspace(lat_min, lat_max, n_points)\nlon_grid, lat_grid = np.meshgrid(lons, lats)"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#calculate-the-distance",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#calculate-the-distance",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Calculate the distance",
    "text": "Calculate the distance\nOnce we have defined the grid, the next step is to calculate the distance from each point on the grid to each source of information. There are two types of distances we can use: Haversine distance andGreat Circle distance.\nHaversine distance is the shortest distance between two points on a sphere. Great Circle distance is another type of distance we can use to calculate the distance between two points on a sphere. It‚Äôs the shortest distance along the great circle, which is the imaginary circle that encircles the sphere and passes through the two points.\nWhich distance we use depends on the specific case. In the case of the Thames, which is a long, winding river, the shortest distance between two points on the river may not be the shortest distance between the two points on the grid.\n\n\nCode\ndef haversine(lon, lat, coords):\n    \n    '''\n    Returns the haversine distance between a pair of coordinates and the coordinates of the grid\n    \n     Parameters\n    ------------\n        lon: int, Longitude of a point in the grid\n        lat: int, Latitude of a point in the grid\n        coords: tuple, Coordinates of the piece of information\n            \n    Return\n    -----------\n        distance : int\n            Difference between pair of coordinates and the coordinates of the grid \n    ''' \n    \n    lon1, lat1 = coords\n    R = 6371  # Earth radius in km\n    diff_lat = np.radians(lat1 - lat)\n    diff_lon = np.radians(lon1 - lon)\n    a = np.sin(diff_lat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat)) * np.sin(diff_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    return distance\n\n\nThe Great Circle distance, on the other hand, is another type of distance we can use to calculate the distance between two points on a sphere. It‚Äôs the shortest distance along the great circle, which is the imaginary circle that encircles the sphere and passes through the two points.\nThis type of distance can be a good alternative in cases where there is not a clear trajectory between the points. In the case of the Thames which is a long, winding river, the shortest distance between two points on the river may not be the shortest distance between the two points on the grid.\n\n\nCode\ndef great_circle_distance(lon, lat, coords):\n    \n    '''\n    Returns the great circle distance between a pair of coordinates and the coordinates of the grid\n    \n     Parameters\n    ------------\n        lon: int, Longitude of a point in the grid\n        lat: int, Latitude of a point in the grid\n        coords: tuple, Coordinates of the piece of information\n            \n    Return\n    -----------\n        distance : int\n            Difference between pair of coordinates and the coordinates of the grid \n    '''\n    \n    lon1, lat1 = coords[0]\n    lon2, lat2 = coords[1]\n    R = 6371  # Earth radius in km\n    diff_lat = np.radians(lat2 - lat1)\n    diff_lon = np.radians(lon2 - lon1)\n    a = np.sin(diff_lat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(diff_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    # Calculate the distance between each point on the grid and the great circle path\n    distance_to_path = np.abs(distance * np.sin(np.radians(lon) - np.radians(coords[0][0])))\n    return distance_to_path\n\n\n\nCalculate the probabilities\nOnce we have the distances from the suspect to each point on the grid, we need to calculate the probability of the suspect being at each point. To do this, we can use a statistical distribution, such as a normal distribution or alognormal distribution.\nA normal distribution is bell-shaped, with the highest probability in the center and the probability decreasing as you move away from the center.\n\n\nCode\n# Probability functions \ndef gaussian_prob(distance_to_thames):\n    '''\n    Returns a gaussian probability\n    \n     Parameters\n    ------------\n        distance_to_thames: int, distance (from the grid point) to thames (point)\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to thames\n            \n    '''\n\n    pdf = norm.pdf(distance_to_thames, loc=0, scale=2730)\n    return pdf\n\n\n\ndef normal_prob(distance_to_satellite_path):\n    '''\n    Returns a lognormal probability\n    \n     Parameters\n    ------------\n        distance_to_satellite: int, distance (from the grid point) to points of the satellite path\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to the satellite points of the path\n            \n    '''\n    pdf = norm.pdf(distance_to_satellite_path, loc=0, scale=3160)\n    return pdf\n\n\nA lognormal distribution, on the other hand, is skewed to the right, with a few very high values and many lower values.\nThe type of distribution we use depends on the nature of the data. In the case of the Bank of England coordinates, which are likely to be skewed to the right, we might use a lognormal distribution.\n\n\nCode\ndef lognormal_prob(distance_to_boe):\n    '''\n    Returns a lognormal probability\n    \n     Parameters\n    ------------\n        distance_to_boe: int, distance (from the grid point) to Bank of England (point)\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to Bank of England\n            \n    '''\n    pdf = lognorm.pdf(distance_to_boe, s=0.625, scale=np.exp(8.460))\n    return pdf\n\n\nOnce we have calculated the probability of the suspect being at each point on the grid, we need to combine these probabilities to get a single probability distribution for the suspect‚Äôs location.\n\n\nCode\n#Calculate distances and probabilities to each source\n\n# THAMES -----------------------------------\n\n\ndistance_to_thames = np.zeros((n_points, n_points))\n\nfor i in range(len(lons)):\n    for j in range(len(lats)):\n        # Calculate the distance from each point on the grid to the Thames\n        # using the Haversine formula for spherical geometry\n        distance_to_thames[j, i] = haversine(lon_grid[j, i], lat_grid[j, i], thames_coordinates[0])\n\nprob_thames = gaussian_prob(distance_to_thames)\n\n# BANK OF ENGLAND  --------------------------------\n\ndistance_to_boe = np.sqrt((lon_grid - boe_coordinates[1])**2 + (lat_grid - boe_coordinates[0])**2)\nprob_boe = lognormal_prob(distance_to_boe) \n\n# SATELLITE ----------------------------------------\n\ndistance_to_satellite_path = great_circle_distance(lon_grid, lat_grid, sat_path_coords)\nprob_satellite = normal_prob(distance_to_satellite_path)\n\n\n\n\nCombine the probabilities for each point\nOnce we have calculated the probability of the suspect being at each point on the grid, we can combine these probabilities to get a single probability distribution for the suspect‚Äôs location.\nOne way to combine probabilities is to use a stacking ensemble method. Stacking ensemble methods combine the predictions of multiple machine learning models to produce a more accurate prediction.\nIn tis case, we can do this by training a Machine Learning method, RandomForest(), on the probabilities from the three sources of information to predict the (most likely) coordinates that compose the grid. Since this type of output is based on two elements (latitude and longitude), the random forest requires a MultiOutputRegressor()\n\n\nCode\n# Combine the three sets of probabilities into a single input array\nX = np.stack([prob_thames.flatten(), prob_boe.flatten(), prob_satellite.flatten()], axis=1)\n\n# Create the target array of grid points\nY = np.stack([lon_grid.flatten(), lat_grid.flatten()], axis=1)\n\n# Split the data into training and testing sets\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Create a multi-output RandomForestClassifier\n\nclf = RandomForestRegressor(n_estimators=100, random_state=42)\nmulti_clf = MultiOutputRegressor(clf)\n\n\n\n# Fit the classifier to the training data\nmulti_clf.fit(X_train, Y_train)\n\n\n# Predict the grid points for the test data\nY_pred = multi_clf.predict(X_test)\nrmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n\n\ndisplay(f\"The most likely location is {Y_pred[1]} and a root mean squared error (RMSE) on test set: {rmse:.4f}\")\n\n\n'The most likely location is [-0.0699982 51.4999544] and a root mean squared error (RMSE) on test set: 0.0239'\n\n\nThe coordinates where researcher is more likely to be are [51.4999544, -0.0699982]. Since we are running a regression model the evaluation of our results is on the basis of the Mean Square Error (MSE). The model provides which renders a 0.0239"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#visualise-the-results",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#visualise-the-results",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Visualise the results",
    "text": "Visualise the results\nOnce we have the predicted probabilities, we can visualize them using a library like folium. This will show us the most likely location of the suspect.\n\n\nCode\nlondon_coords = [51.509865, -0.118092]\nresearcher_coords = [51.4999544,-0.0699982]\nboe_coords = [51.514171,-0.088438]\n\nlondon_map = folium.Map(location=london_coords, zoom_start=12.5)\n\n\n\nfolium.Marker(\n    location= researcher_coords,\n    popup=\"Researcher likely location\",\n    icon=folium.Icon(color=\"red\"),\n).add_to(london_map)\n\nfolium.Circle(\n    radius=500,\n    location= researcher_coords,\n    popup=\"Researcher likely location\",\n    fill=False,\n).add_to(london_map)\n\n\nfolium.Marker(\n    location= boe_coords,\n    popup=\"Bank of England\",\n    icon=folium.Icon(color=\"green\", icon=\"building-columns\"),\n).add_to(london_map)\n\n\n\nlondon_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/2023_10_12_use_ml_to_locate_data/index.html#conclusion",
    "href": "posts/2023_10_12_use_ml_to_locate_data/index.html#conclusion",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Conclusion",
    "text": "Conclusion\nBy using machine learning, we can estimate a likely location in a map quickly and efficiently even in cases where there is limited information."
  },
  {
    "objectID": "posts/2023_10_17_use_ml_to_locate_data/index.html",
    "href": "posts/2023_10_17_use_ml_to_locate_data/index.html",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "",
    "text": "Imagine you‚Äôre a detective trying to track down a suspect in London. You have a few leads, but nothing concrete. One says the suspect was seen near the Bank of England. Another says it is near the Thames River. And a third says it may be within 3160 meters of a particular satellite path.\nCan you use machine learning to find the suspect? Yes, you can. You can do so by estimating the most likely point within a grid that reflects all possible locations where the suspect can be located.\nIn this blog I will show how to approach this problem from a machine learning perspective"
  },
  {
    "objectID": "posts/2023_10_17_use_ml_to_locate_data/index.html#defining-a-grid",
    "href": "posts/2023_10_17_use_ml_to_locate_data/index.html#defining-a-grid",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Defining a grid",
    "text": "Defining a grid\nFirst, we need to define a grid that covers all possible locations where the suspect could be. We can do this using a meshgrid() function in NumPy. This function creates a two-dimensional grid of points, where each point is defined by a pair of coordinates.\nWe can set the boundaries of the grid to be the coordinates of the River Thames, the Bank of England, and the satellite path. This will ensure that the grid covers all possible locations where the suspect could be.\n\n\nCode\nimport numpy as np # for data manipulation \nfrom sklearn.ensemble import RandomForestRegressor   # for modelling \nfrom sklearn.multioutput import MultiOutputRegressor # for modelling \nfrom sklearn.metrics import mean_squared_error       # for evaluation\nfrom sklearn.model_selection import train_test_split # for modelling \nimport folium # for viz\n# Packages for functions -----------------------------------\nimport numpy as np\nfrom scipy.stats import norm, lognorm\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n# Define the set of coordinates ---------\nthames_coordinates = [\n    (51.489467, -0.236313), (51.468045, -0.216379),\n    (51.464141, -0.190458), (51.473257, -0.179515),\n    (51.480661, -0.173850), (51.484590, -0.148573),\n    (51.483601, -0.137501), (51.485793, -0.129604),\n    (51.494744, -0.122824), (51.508208, -0.118489),\n    (51.509330, -0.096431), (51.501904, -0.058365),\n    (51.508662, -0.043216), (51.506098, -0.030727),\n    (51.490202, -0.028796), (51.485098, -0.007725),\n    (51.490683, 0.000215), (51.502305, -0.005407),\n    (51.506552, 0.005536)\n]\n\n\nboe_coordinates = [51.514171,-0.088438]\n\nsat_path_coords = [(51.451000, -0.300000), (51.560000, 0.000000)]\n\n\n\n\n# Define the grid of points ---------------\nlon_min, lon_max = -0.1, -0.04\nlat_min, lat_max = 51.45, 51.55\n\nn_points = 500\nlons = np.linspace(lon_min, lon_max, n_points)\nlats = np.linspace(lat_min, lat_max, n_points)\nlon_grid, lat_grid = np.meshgrid(lons, lats)"
  },
  {
    "objectID": "posts/2023_10_17_use_ml_to_locate_data/index.html#calculate-the-distance",
    "href": "posts/2023_10_17_use_ml_to_locate_data/index.html#calculate-the-distance",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Calculate the distance",
    "text": "Calculate the distance\nOnce we have defined the grid, the next step is to calculate the distance from each point on the grid to each source of information. There are two types of distances we can use: Haversine distance andGreat Circle distance.\nHaversine distance is the shortest distance between two points on a sphere. Great Circle distance is another type of distance we can use to calculate the distance between two points on a sphere. It‚Äôs the shortest distance along the great circle, which is the imaginary circle that encircles the sphere and passes through the two points.\nWhich distance we use depends on the specific case. In the case of the Thames, which is a long, winding river, the shortest distance between two points on the river may not be the shortest distance between the two points on the grid.\n\n\nCode\ndef haversine(lon, lat, coords):\n    \n    '''\n    Returns the haversine distance between a pair of coordinates and the coordinates of the grid\n    \n     Parameters\n    ------------\n        lon: int, Longitude of a point in the grid\n        lat: int, Latitude of a point in the grid\n        coords: tuple, Coordinates of the piece of information\n            \n    Return\n    -----------\n        distance : int\n            Difference between pair of coordinates and the coordinates of the grid \n    ''' \n    \n    lon1, lat1 = coords\n    R = 6371  # Earth radius in km\n    diff_lat = np.radians(lat1 - lat)\n    diff_lon = np.radians(lon1 - lon)\n    a = np.sin(diff_lat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat)) * np.sin(diff_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    return distance\n\n\nThe Great Circle distance, on the other hand, is another type of distance we can use to calculate the distance between two points on a sphere. It‚Äôs the shortest distance along the great circle, which is the imaginary circle that encircles the sphere and passes through the two points.\nThis type of distance can be a good alternative in cases where there is not a clear trajectory between the points. In the case of the Thames which is a long, winding river, the shortest distance between two points on the river may not be the shortest distance between the two points on the grid.\n\n\nCode\ndef great_circle_distance(lon, lat, coords):\n    \n    '''\n    Returns the great circle distance between a pair of coordinates and the coordinates of the grid\n    \n     Parameters\n    ------------\n        lon: int, Longitude of a point in the grid\n        lat: int, Latitude of a point in the grid\n        coords: tuple, Coordinates of the piece of information\n            \n    Return\n    -----------\n        distance : int\n            Difference between pair of coordinates and the coordinates of the grid \n    '''\n    \n    lon1, lat1 = coords[0]\n    lon2, lat2 = coords[1]\n    R = 6371  # Earth radius in km\n    diff_lat = np.radians(lat2 - lat1)\n    diff_lon = np.radians(lon2 - lon1)\n    a = np.sin(diff_lat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(diff_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    # Calculate the distance between each point on the grid and the great circle path\n    distance_to_path = np.abs(distance * np.sin(np.radians(lon) - np.radians(coords[0][0])))\n    return distance_to_path\n\n\n\nCalculate the probabilities\nOnce we have the distances from the suspect to each point on the grid, we need to calculate the probability of the suspect being at each point. To do this, we can use a statistical distribution, such as a normal distribution or alognormal distribution.\nA normal distribution is bell-shaped, with the highest probability in the center and the probability decreasing as you move away from the center.\n\n\nCode\n# Probability functions \ndef gaussian_prob(distance_to_thames):\n    '''\n    Returns a gaussian probability\n    \n     Parameters\n    ------------\n        distance_to_thames: int, distance (from the grid point) to thames (point)\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to thames\n            \n    '''\n\n    pdf = norm.pdf(distance_to_thames, loc=0, scale=2730)\n    return pdf\n\n\n\ndef normal_prob(distance_to_satellite_path):\n    '''\n    Returns a lognormal probability\n    \n     Parameters\n    ------------\n        distance_to_satellite: int, distance (from the grid point) to points of the satellite path\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to the satellite points of the path\n            \n    '''\n    pdf = norm.pdf(distance_to_satellite_path, loc=0, scale=3160)\n    return pdf\n\n\nA lognormal distribution, on the other hand, is skewed to the right, with a few very high values and many lower values.\nThe type of distribution we use depends on the nature of the data. In the case of the Bank of England coordinates, which are likely to be skewed to the right, we might use a lognormal distribution.\n\n\nCode\ndef lognormal_prob(distance_to_boe):\n    '''\n    Returns a lognormal probability\n    \n     Parameters\n    ------------\n        distance_to_boe: int, distance (from the grid point) to Bank of England (point)\n                \n    Return\n    -----------\n        pdf : int, probability distribution of the distance to Bank of England\n            \n    '''\n    pdf = lognorm.pdf(distance_to_boe, s=0.625, scale=np.exp(8.460))\n    return pdf\n\n\nOnce we have calculated the probability of the suspect being at each point on the grid, we need to combine these probabilities to get a single probability distribution for the suspect‚Äôs location.\n\n\nCode\n#Calculate distances and probabilities to each source\n\n# THAMES -----------------------------------\n\n\ndistance_to_thames = np.zeros((n_points, n_points))\n\nfor i in range(len(lons)):\n    for j in range(len(lats)):\n        # Calculate the distance from each point on the grid to the Thames\n        # using the Haversine formula for spherical geometry\n        distance_to_thames[j, i] = haversine(lon_grid[j, i], lat_grid[j, i], thames_coordinates[0])\n\nprob_thames = gaussian_prob(distance_to_thames)\n\n# BANK OF ENGLAND  --------------------------------\n\ndistance_to_boe = np.sqrt((lon_grid - boe_coordinates[1])**2 + (lat_grid - boe_coordinates[0])**2)\nprob_boe = lognormal_prob(distance_to_boe) \n\n# SATELLITE ----------------------------------------\n\ndistance_to_satellite_path = great_circle_distance(lon_grid, lat_grid, sat_path_coords)\nprob_satellite = normal_prob(distance_to_satellite_path)\n\n\n\n\nCombine the probabilities for each point\nOnce we have calculated the probability of the suspect being at each point on the grid, we can combine these probabilities to get a single probability distribution for the suspect‚Äôs location.\nOne way to combine probabilities is to use a stacking ensemble method. Stacking ensemble methods combine the predictions of multiple machine learning models to produce a more accurate prediction.\nIn this case, we can do this by training a Machine Learning method, RandomForest(), on the probabilities from the three sources of information to predict the (most likely) coordinates that compose the grid. Since this type of output is based on two elements (latitude and longitude), the random forest requires a MultiOutputRegressor()\n\n\nCode\n# Combine the three sets of probabilities into a single input array\nX = np.stack([prob_thames.flatten(), prob_boe.flatten(), prob_satellite.flatten()], axis=1)\n\n# Create the target array of grid points\nY = np.stack([lon_grid.flatten(), lat_grid.flatten()], axis=1)\n\n# Split the data into training and testing sets\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# Create a multi-output RandomForestClassifier\n\nclf = RandomForestRegressor(n_estimators=100, random_state=42)\nmulti_clf = MultiOutputRegressor(clf)\n\n\n\n# Fit the classifier to the training data\nmulti_clf.fit(X_train, Y_train)\n\n\n# Predict the grid points for the test data\nY_pred = multi_clf.predict(X_test)\nrmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n\n\ndisplay(f\"The most likely location is {Y_pred[1]} and a root mean squared error (RMSE) on test set: {rmse:.4f}\")\n\n\n'The most likely location is [-0.0699982 51.4999544] and a root mean squared error (RMSE) on test set: 0.0239'\n\n\nThe coordinates where the suspect is more likely to be are [51.4999544, -0.0699982]. Since we are running a regression model we can evaluate the results of the model on the basis of the Root Mean Square Error (RMSE) (0.0239)"
  },
  {
    "objectID": "posts/2023_10_17_use_ml_to_locate_data/index.html#visualise-the-results",
    "href": "posts/2023_10_17_use_ml_to_locate_data/index.html#visualise-the-results",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Visualise the results",
    "text": "Visualise the results\nOnce we have the predicted probabilities, we can visualize them using a library like folium. This will show us the most likely location of the suspect.\n\n\nCode\nlondon_coords = [51.509865, -0.118092]\nsuspect_coords = [51.4999544,-0.0699982]\nboe_coords = [51.514171,-0.088438]\n\nlondon_map = folium.Map(location=london_coords, zoom_start=12.5)\n\n\n\nfolium.Marker(\n    location= suspect_coords,\n    popup=\"Suspect likely location\",\n    icon=folium.Icon(color=\"red\"),\n).add_to(london_map)\n\nfolium.Circle(\n    radius=500,\n    location= suspect_coords,\n    popup=\"Suspect likely location\",\n    fill=False,\n).add_to(london_map)\n\n\nfolium.Marker(\n    location= boe_coords,\n    popup=\"Bank of England\",\n    icon=folium.Icon(color=\"green\", icon=\"building-columns\"),\n).add_to(london_map)\n\n\n\nlondon_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/2023_10_17_use_ml_to_locate_data/index.html#conclusion",
    "href": "posts/2023_10_17_use_ml_to_locate_data/index.html#conclusion",
    "title": "How to find a needle in a haystack using machine learning",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog we have seen an example of how we can use machine learning to estimate a likely location in a map quickly and efficiently even in cases where there is limited information."
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html",
    "title": "2023 Bike Packing Wrap Up",
    "section": "",
    "text": "Is this time of the year where we receive the wrap ups with the highlights of our activities. 2023 has lived a great rise bike packing events all over the world. According to DotWatcher, a UK based company building up a great community around this sport, bike-packing racing is living its own golden age and there has been a rise on the events hosted. No matter what discipline, either road, off-road or gravel, there have been events all over the world.\nUsing information about riders that posted on both DotWatcher and Bikepacking.com information about their experiences, in this blog I am going to show some insights of what has been 2023 in terms of bike packing races."
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#the-set-up",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#the-set-up",
    "title": "2023 Bike Packing Wrap Up",
    "section": "The set up",
    "text": "The set up\nPrior to visualise the insights, a quick note on the data. Data are extracted from sections associated with Rigs of and Bikes of from Bikepacking.com and Dotwatcher.cc\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\n\n\ndf_bikepacking = read_csv('data/app_df.csv')\nkable(df_bikepacking %&gt;% head(5))\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nage_group\nrace\nbike_brand\nbike_models\ntype_race\n\n\n\n\nAustria\n30 - 40 years old\nHellenic Mountain Race 2023\nSpecialized\nEpic\noff_road\n\n\nUnited States\n50 - 60 years old\nStagecoach 400 2023\nMarin\nRift Zone 2\noff_road\n\n\nUnited States\n50 - 60 years old\nStagecoach 400 2023\nCanyon\nLux Trail\noff_road\n\n\nCanada\n50 - 60 years old\nStagecoach 400 2023\nHabanero\nCustom 4XL Titanium Hardtail\noff_road\n\n\nItaly\n40 - 50 years old\nHellenic Mountain Race 2023\nCanyon\nLux Trail CF7 Columbia Blue 2022\noff_road"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#exploring-the-data",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#exploring-the-data",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Exploring the data",
    "text": "Exploring the data\nMost of the info provided by riders is associated with Gravel events (42%) followed by Off-Road (35%) and Road (23%). Within gravel, Badlands obtained\n\n\nCode\nlibrary(purrr)\nlibrary(highcharter)\nlibrary(htmlwidgets)\n\n\nWarning: package 'htmlwidgets' was built under R version 4.3.1\n\n\nCode\nby_type = df_bikepacking |&gt;\n  count(type_race)\n\n\npie_chart&lt;- by_type|&gt;\n#set up highchart object\n  hchart(\"pie\", \n         #mapping for pie chart\n         hcaes(x = type_race, y = n, drilldown=type_race), \n         name=\"Total riders\")|&gt;\n  #add title\n  hc_title(text=\"By Race Type\") |&gt; hc_plotOptions(pie = list(innerSize=\"70%\"))\n\n\nby_subtype = df_bikepacking |&gt;\n  count(type_race, race) |&gt;\n  group_nest(type_race) |&gt;\n  mutate(\n    #id should be set to parent level\n    id = type_race,\n    #type specifies chart type\n    type = \"column\",\n    #drilldown data should contain arguments for chart - use purrr to map\n    data = purrr::map(data, mutate, name = race, y  = n),\n     data = purrr::map(data, list_parse))\n\n\n\ndrilldown_chart&lt;-pie_chart|&gt;\n  hc_drilldown(\n    #map to data\n    series = list_parse(by_subtype),\n    allowPointDrilldown = TRUE,\n    #set stylings of data labels that offer drill down views\n    activeDataLabelStyle = list(\n      textDecoration=\"none\",\n      color=\"black\"\n    )\n  )\n\n\nfinal_chart = drilldown_chart|&gt;\n  #relabel x Axis\n  hc_xAxis(title = list(text=\"\"))|&gt;\n  #relabel y Axis\n  hc_yAxis(title = list(text=\"# of Riders\"))|&gt;\n  #reorder column charts by y Axis\n  hc_plotOptions(column = list(\n                   dataSorting = list(enabled=TRUE)\n                   )\n                 )|&gt;\n  #customize drilldown & drillup events\n  hc_chart(\n           events = list(\n             drilldown = JS(\n               \"function(){\n               this.title.update({text: 'Riders by Race'})\n               this.update({\n                  xAxis:{visible:true},\n                  yAxis:{visible:true}\n               })\n               }\"\n             ),\n             drillup =  JS(\"function() {\n              this.title.update({text: 'Riders by Type of Race'})\n              this.update({\n                xAxis:{visible:false},\n                yAxis:{visible:false}\n               })\n             }\")\n           ))\n\nfinal_chart"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#the-set-up-the-data",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#the-set-up-the-data",
    "title": "2023 Bike Packing Wrap Up",
    "section": "The set up & the Data",
    "text": "The set up & the Data\nPrior to visualise the insights, a quick note on the data. Data are extracted from sections associated with Rigs of and Bikes of from Bikepacking.com and Dotwatcher.cc. These are dedicated sections where riders post information about their bikes, the gear they intend to use in the race, where they come from, etc‚Ä¶\nI have built a scraper that I have combined with a Large Language Model1 to extract information about different elements referred to the bikes and components used by the riders. A snapshot fo th information availables\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nage_group\nrace\nbike_brand\nbike_models\ntype_race\n\n\n\n\nAustria\n30 - 40 years old\nHellenic Mountain Race 2023\nSpecialized\nEpic\nOff Road\n\n\nUnited States\n50 - 60 years old\nStagecoach 400 2023\nMarin\nRift Zone 2\nOff Road\n\n\nUnited States\n50 - 60 years old\nStagecoach 400 2023\nCanyon\nLux Trail\nOff Road\n\n\nCanada\n50 - 60 years old\nStagecoach 400 2023\nHabanero\nCustom 4XL Titanium Hardtail\nOff Road\n\n\nItaly\n40 - 50 years old\nHellenic Mountain Race 2023\nCanyon\nLux Trail CF7 Columbia Blue 2022\nOff Road"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#what-type-of-event-got-more-attention-for-the-riders",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#what-type-of-event-got-more-attention-for-the-riders",
    "title": "2023 Bike Packing Wrap Up",
    "section": "What type of event got more attention for the riders?",
    "text": "What type of event got more attention for the riders?\nRiders were more engaged with gravel events. Out of the almost 2200 riders that reported information about their gear, a 42% did it for an event the info provided by riders is associated with Gravel events (42%) followed by Off-Road (35%) and Road (23%). Within gravel, Badlands led the events with 122 riders reporting their gear. Considering off-road, the Tour Divide obtained most of th was Atlas Mountain Race with 104"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#footnotes",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#footnotes",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe model used is gpt-3.5-turbo-1106‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#what-type-of-riders",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#what-type-of-riders",
    "title": "2023 Bike Packing Wrap Up",
    "section": "What type of riders",
    "text": "What type of riders\n\nAge groups\nGravel seems to be a popular among riders in their 40s and 30s. Off road events look like the event to go as riders get older. This is specially true when riders are in their sixties.\n\n\n\n\n\n\n\n\n\nCountries of the riders\nRiders from the UK are the most numerous followed by German and American riders. There is a distinction in the type of races. Whereas European riders tend to favour gravel races, American, Canadian and Kiwi riders seem to sign up for off-road events more frequently (in 80% - 90% of the total sign-ups of the riders in the country). Road racing was more determining for French and Polish riders who signed up for Transpyrenees and the Race Across Poland respectively"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#what-type-of-event-got-more-attention-from-the-riders",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#what-type-of-event-got-more-attention-from-the-riders",
    "title": "2023 Bike Packing Wrap Up",
    "section": "What type of event got more attention from the riders?",
    "text": "What type of event got more attention from the riders?\nRiders were more engaged with gravel events. Out of the almost 2200 riders that reported information about their gear, a 42% did it for an event the info provided by riders is associated with Gravel events (42%) followed by Off-Road (35%) and Road (23%). Within gravel, Badlands led the events with 122 riders reporting their gear. Considering off-road, the Tour Divide obtained most of th was Atlas Mountain Race with 104"
  },
  {
    "objectID": "posts/2023_12_04_a_year_in_ultracycling/index.html#bikes",
    "href": "posts/2023_12_04_a_year_in_ultracycling/index.html#bikes",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Bikes",
    "text": "Bikes"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html",
    "title": "2023 Bike Packing Wrap Up",
    "section": "",
    "text": "As this year ends, I look back at the highlights in the bike-packing world. The year 2023 has seen a significant rise in bike-packing events globally. According to DotWatcher, a UK-based company that‚Äôs building a strong community around this sport, bike-packing racing is experiencing a golden age with an increase in events. Whether it‚Äôs road, off-road, or gravel, there‚Äôs been a noticeable growth in these activities worldwide during the last year.\nIn this blog, I‚Äôll share insights about the 2023 bike-packing scene, using information from riders1 who participated in various events and shared their experiences and gear details on DotWatcher and Bikepacking.com"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#the-set-up-the-data",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#the-set-up-the-data",
    "title": "2023 Bike Packing Wrap Up",
    "section": "The set up & the Data",
    "text": "The set up & the Data\nBefore we dive into the insights, it‚Äôs important to understand the source of our data. The information is gathered from Rigs of and Bikes of sections. These are specific sections in Bikepacking.com and Dotwatcher.cc where where riders share details about their bikes and gear for races.\n\n\n\nExample of section Bikes of from Dotwatcher.cc\n\n\nThe real challenge lies in extracting meaningful information from these diverse and detailed submissions. For instance, we‚Äôre looking at specifics like bike models, bag types, and wheel choices.\nTo tackle this, I‚Äôve developed a tool that combines web scraping techniques with advanced language processing2. This approach is especially effective for sorting through and making sense of unstructured data. It‚Äôs not just about collecting data; it‚Äôs about finding the patterns and preferences that really define the bike-packing community in 2023. Let‚Äôs explore what these data reveal about the choices and trends among bike-packers this year."
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#what-type-of-event-got-more-attention-from-the-riders",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#what-type-of-event-got-more-attention-from-the-riders",
    "title": "2023 Bike Packing Wrap Up",
    "section": "What type of event got more attention from the riders?",
    "text": "What type of event got more attention from the riders?\nRiders were more engaged with gravel events. Out of the almost 2200 riders that reported information about their gear, a 42% did it for an event the info provided by riders is associated with Gravel events (42%) followed by Off-Road (35%) and Road (23%). Within gravel, Badlands led the events with 122 riders reporting their gear. Considering off-road, the Tour Divide obtained most of th was Atlas Mountain Race with 104"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#what-type-of-riders",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#what-type-of-riders",
    "title": "2023 Bike Packing Wrap Up",
    "section": "What type of riders",
    "text": "What type of riders\n\nAge groups\n\nIn cycling events, age demographics show distinct preferences. The majority of participants are in their thirties, and gravel events, in particular, have a consistent appeal to riders in their 30s to 50s. Conversely, off-road events seem to attract the younger and older segments of cyclists more.\nLooking more closely at the data, the most engaging gravel events for riders in their 30s include Bright Midnight, with 46 participants, Badlands with 38, and Seven Serpents attracting 35 riders. For the age group of 40 to 50, Badlands emerges as a notable choice, though other events like Bright Midnight, Seven Serpents, Basajaun, and Istra Land also report comparable participation. These trends suggest a balanced interest in gravel cycling across various age groups.\n\n\n\n\n\n\n\n\n\nCountries of the riders\nRiders from the UK are the most numerous followed by German and American riders. There is a distinction in the type of races. Whereas European riders tend to favour gravel races, American, Canadian and Kiwi riders seem to sign up for off-road events more frequently (in 80% - 90% of the total sign-ups of the riders in the country). Road racing was more determining for French and Polish riders who signed up for Transpyrenees and the Race Across Poland respectively"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#bikes",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#bikes",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Bikes",
    "text": "Bikes\nCanyon and Specialized have been the top brands chosen by the riders regardless of the type of race. 8% of the reported brands for Gravel was a Canyon whereas Specialized was a 6%. For off-road, Salsa led the choices of the riders in 11% of the cases followed by Canyon and Specialized with a 6% each. Finally, in terms of road racing, Canyon was in a 9% of the cases."
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#footnotes",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#footnotes",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDisclaimer: This sample does not represent the entire population participating in bikepacking events but only those riders that voluntarily post information about their gear.‚Ü©Ô∏é\nTo tackle this task, I am using gpt-3.5-turbo-1106.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#what-bikes-were-used",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#what-bikes-were-used",
    "title": "2023 Bike Packing Wrap Up",
    "section": "What bikes were used?",
    "text": "What bikes were used?\nCanyon and Specialized have been the top brands chosen by the riders regardless of the type of race. 8% of the reported brands for Gravel was a Canyon whereas Specialized was a 6%. For off-road, Salsa led the choices of the riders in 11% of the cases followed by Canyon and Specialized with a 6% each. Finally, in terms of road racing, Canyon was in a 9% of the case followed by Specialized and Cannondale with both 5%"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#analyzing-bikepacking-trends-the-data",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#analyzing-bikepacking-trends-the-data",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Analyzing bikepacking trends: The Data",
    "text": "Analyzing bikepacking trends: The Data\nBefore we dive into the insights, it‚Äôs important to understand the source of our data. The information is gathered from Rigs of and Bikes of sections. These are specific sections in Bikepacking.com and Dotwatcher.cc where where riders share details about their bikes and gear for races.\n\n\n\nExample of section Bikes of from Dotwatcher.cc\n\n\nThe real challenge lies in extracting meaningful information from these diverse and detailed submissions. For instance, we‚Äôre looking at specifics like bike models, bag types, and wheel choices.\nTo tackle this, I‚Äôve developed a tool that combines web scraping techniques with advanced language processing2. This approach is especially effective for sorting through and making sense of unstructured data. It‚Äôs not just about collecting data; it‚Äôs about finding the patterns and preferences that really define the bike-packing community in 2023.\nBelow is a table offering a snapshot of what this data reveals:\n\n\n\n\n\n\n  \n    \n    \n      Country\n      Age Group\n      Race\n      Brand\n      Model\n      Type of Race\n    \n  \n  \n    Austria\n30 - 40 years old\nHellenic Mountain Race 2023\nSpecialized\nEpic\nOff Road\n    United States\n50 - 60 years old\nStagecoach 400 2023\nMarin\nRift Zone 2\nOff Road\n    United States\n50 - 60 years old\nStagecoach 400 2023\nCanyon\nLux Trail\nOff Road\n    Canada\n50 - 60 years old\nStagecoach 400 2023\nHabanero\nCustom 4XL Titanium Hardtail\nOff Road\n    Italy\n40 - 50 years old\nHellenic Mountain Race 2023\nCanyon\nLux Trail CF7 Columbia Blue 2022\nOff Road"
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#which-events-captured-the-most-interest-among-riders",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#which-events-captured-the-most-interest-among-riders",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Which Events Captured the Most Interest Among Riders?",
    "text": "Which Events Captured the Most Interest Among Riders?\nGravel events seem to be increasingly capturing the attention of riders. From the nearly 2200 riders that shared details about their gear choices, 42% of them were associated with gravel events, outshining Off-Road events (35%) and Road events (23%) in engagement.\n\n\n\n\n\n\n\nLeading the gravel category is Badlands race. About 122 riders reported their gear. Following close there are two new races introduced in 2023, Bright Midnight and Seven Serpents, which attracted 78 and 70 riders to post their choices respectively.\nFor off-road cyclists, the classic Tour Divide was the most popular, with 126 riders providing gear information. Other notable events include the Atlas Mountain Race with 104 participants, the Silk Mountain Race with 79, and the Hellenic Mountain Race with 59. All these events are organized by the same director, adding a unique consistency to their appeal.\nIn the Road racing category, the Pan Celtic Race is at the forefront, with 73 riders sharing their gear choices. This is followed by the Mittelgerbirge Classique, which drew the interest of 58 riders, and the Three Peaks Bike Race with 46 participants."
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#rider-demographics",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#rider-demographics",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Rider demographics",
    "text": "Rider demographics\n\nAge distribution\n\nIn cycling events, age demographics show distinct preferences. The majority of participants are in their thirties, and gravel events, in particular, have a consistent appeal to riders in their 30s to 50s. Conversely, off-road events seem to attract the younger and older segments of cyclists more.\nLooking more closely at the data, the most engaging gravel events for riders in their 30s include Bright Midnight, with 46 participants, Badlands with 38, and Seven Serpents attracting 35 riders. For the age group of 40 to 50, Badlands emerges as a notable choice, though other events like Bright Midnight, Seven Serpents, Basajaun, and Istra Land also report comparable participation. These trends suggest a balanced interest in gravel cycling across various age groups.\n\n\n\n\n\n\n\n\n\nNationalities of the Participants\nThe highest number of riders hail from the UK, with Germany and the USA also contributing significantly to the participant pool. However, there‚Äôs a notable difference in their event preferences.\nEuropean riders, particularly from the UK and Germany, show a stronger inclination towards gravel races. In contrast, riders from the USA, Canada, and New Zealand predominantly opt for off-road events, representing between 80% to 90% of entries from these countries.\nIn the realm of road racing, French and Polish cyclists show a distinct preference, with many choosing to participate in the Transpyrenees and the Race Through Poland, respectively."
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#bike-brand-preferences",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#bike-brand-preferences",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Bike Brand preferences",
    "text": "Bike Brand preferences\nWhen it comes to the choice of bikes, Canyon and Specialized emerge as the leading brands among riders across various race types. In gravel events, Canyon bikes were used by 8% of the riders, while Specialized bikes accounted for 6%. In the off-road category, Salsa took the lead, being the choice for 11% of the riders, with Canyon and Specialized each capturing 6%. For road racing, Canyon again led the preference with 9%, followed closely by Specialized and Cannondale, each with a 5% usage rate among participants.\n\n\n\n\n\n\n\nAmong the most popular bike brands, the Canyon Grizl, Specialized Diverge, Salsa Cutthroat, Cannondale Topstone, and Trek Procaliber stood out as the top models chosen by riders. These models represent the leading choices within their respective brands."
  },
  {
    "objectID": "posts/2023_12_13_a_year_in_ultracycling/index.html#conclusion",
    "href": "posts/2023_12_13_a_year_in_ultracycling/index.html#conclusion",
    "title": "2023 Bike Packing Wrap Up",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has offered a comprehensive look at the 2023 bike-packing scene, using interactive data and insights from the rider community. The trends and preferences highlighted here not only reflect the growing popularity of bike-packing but also the evolving dynamics of this exciting sport.\nStay tuned for more updates and analyses as we continue to explore the world of bike-packing."
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "",
    "text": "As this year ends, I look back at the highlights in the bike-packing world. The year 2023 has seen a significant rise in bike-packing events globally. According to DotWatcher, a UK-based company that‚Äôs building a strong community around this sport, bike-packing racing is experiencing a golden age with an increase in events. Whether it‚Äôs road, off-road, or gravel, there‚Äôs been a noticeable growth in these activities worldwide during the last year.\nIn this blog, I‚Äôll share insights about the 2023 bike-packing scene, using information from riders1 who participated in various events and shared their experiences and gear details on DotWatcher and Bikepacking.com"
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html#analyzing-bikepacking-trends-the-data",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html#analyzing-bikepacking-trends-the-data",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "Analyzing bikepacking trends: The Data",
    "text": "Analyzing bikepacking trends: The Data\nBefore we dive into the insights, it‚Äôs important to understand the source of our data. The information is gathered from Rigs of and Bikes of sections. These are specific sections in Bikepacking.com and Dotwatcher.cc where where riders share details about their bikes and gear for races.\n\n\n\nExample of section Bikes of from Dotwatcher.cc\n\n\nThe real challenge lies in extracting meaningful information from these diverse and detailed submissions. For instance, we‚Äôre looking at specifics like bike models, bag types, and wheel choices.\nTo tackle this, I‚Äôve developed a tool that combines web scraping techniques with advanced language processing2. This approach is especially effective for sorting through and making sense of unstructured data. It‚Äôs not just about collecting data; it‚Äôs about finding the patterns and preferences that really define the bike-packing community in 2023.\nBelow is a table offering a snapshot of what these data reveal:\n\n\n\n\n\n\n  \n    \n    \n      Country\n      Age Group\n      Race\n      Brand\n      Model\n      Type of Race\n    \n  \n  \n    Austria\n30 - 40 years old\nHellenic Mountain Race 2023\nSpecialized\nEpic\nOff Road\n    United States\n50 - 60 years old\nStagecoach 400 2023\nMarin\nRift Zone 2\nOff Road\n    United States\n50 - 60 years old\nStagecoach 400 2023\nCanyon\nLux Trail\nOff Road\n    Canada\n50 - 60 years old\nStagecoach 400 2023\nHabanero\nCustom 4XL Titanium Hardtail\nOff Road\n    Italy\n40 - 50 years old\nHellenic Mountain Race 2023\nCanyon\nLux Trail CF7 Columbia Blue 2022\nOff Road"
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html#which-events-captured-the-most-interest-among-riders",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html#which-events-captured-the-most-interest-among-riders",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "Which Events Captured the Most Interest Among Riders?",
    "text": "Which Events Captured the Most Interest Among Riders?\nGravel events seem to be increasingly capturing the attention of riders. From the nearly 2200 riders that shared details about their gear choices, 42% of them were associated with gravel events, outshining Off-Road events (35%) and Road events (23%) in engagement.\n\n\n\n\n\n\n\nLeading the gravel category is Badlands race. About 122 riders reported their gear. Following close there are two new races introduced in 2023, Bright Midnight and Seven Serpents, which attracted 78 and 70 riders to post their choices respectively.\nFor off-road cyclists, the classic Tour Divide was the most popular, with 126 riders providing gear information. Other notable events include the Atlas Mountain Race with 104 participants, the Silk Mountain Race with 79, and the Hellenic Mountain Race with 59. All these events are organized by the same director, adding a unique consistency to their appeal.\nIn the Road racing category, the Pan Celtic Race is at the forefront, with 73 riders sharing their gear choices. This is followed by the Mittelgerbirge Classique, which drew the interest of 58 riders, and the Three Peaks Bike Race with 46 participants."
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html#rider-demographics",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html#rider-demographics",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "Rider demographics",
    "text": "Rider demographics\n\nAge distribution\n\nIn cycling events, age demographics show distinct preferences. The majority of participants are in their thirties, and gravel events, in particular, have a consistent appeal to riders in their 30s to 50s. Conversely, off-road events seem to attract the younger and older segments of cyclists more.\nLooking more closely at the data, the most engaging gravel events for riders in their 30s include Bright Midnight, with 46 participants, Badlands with 38, and Seven Serpents attracting 35 riders. For the age group of 40 to 50, Badlands emerges as a notable choice, though other events like Bright Midnight, Seven Serpents, Basajaun, and Istra Land also report comparable participation. These trends suggest a balanced interest in gravel cycling across various age groups.\n\n\n\n\n\n\n\n\n\nNationalities of the Participants\nThe highest number of riders hail from the UK, with Germany and the USA also contributing significantly to the participant pool. However, there‚Äôs a notable difference in their event preferences.\nEuropean riders, particularly from the UK and Germany, show a stronger inclination towards gravel races. In contrast, riders from the USA, Canada, and New Zealand predominantly opt for off-road events, representing between 80% to 90% of entries from these countries.\nIn the realm of road racing, French and Polish cyclists show a distinct preference, with many choosing to participate in the Transpyrenees and the Race Through Poland, respectively."
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html#bike-brand-preferences",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html#bike-brand-preferences",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "Bike Brand preferences",
    "text": "Bike Brand preferences\nWhen it comes to the choice of bikes, Canyon and Specialized emerge as the leading brands among riders across various race types. In gravel events, Canyon bikes were used by 8% of the riders, while Specialized bikes accounted for 6%. In the off-road category, Salsa took the lead, being the choice for 11% of the riders, with Canyon and Specialized each capturing 6%. For road racing, Canyon again led the preference with 9%, followed closely by Specialized and Cannondale, each with a 5% usage rate among participants.\n\n\n\n\n\n\n\nAmong the most popular bike brands, the Canyon Grizl, Specialized Diverge, Salsa Cutthroat, Cannondale Topstone, and Trek Procaliber stood out as the top models chosen by riders. These models represent the leading choices within their respective brands."
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html#conclusion",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html#conclusion",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has offered a brief look at the 2023 bike-packing scene, using interactive data and insights from the rider community. The trends and preferences highlighted here not only reflect the growing popularity of bike-packing but also the evolving dynamics of this exciting sport."
  },
  {
    "objectID": "posts/2023_12_12_a_year_in_ultracycling/index.html#footnotes",
    "href": "posts/2023_12_12_a_year_in_ultracycling/index.html#footnotes",
    "title": "Wrapped 2023 in Ultracycling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDisclaimer: This sample does not represent the entire population participating in bikepacking events but only those riders that voluntarily post information about their gear.‚Ü©Ô∏é\nTo tackle this task, I am using gpt-3.5-turbo-1106.‚Ü©Ô∏é"
  }
]