[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Setting up an AWS environment on Mac",
    "section": "",
    "text": "If you want to create a MLOps project, it is likely that you will interact with services offered by a cloud provider. For doing that you will likely require to link your local machine to any of the instances offered by the cloud provider. In this post, we will see how to set up the environment for a Mac machine to work with an EC2 instance on AWS."
  },
  {
    "objectID": "posts/post-with-code/index.html#footnotes",
    "href": "posts/post-with-code/index.html#footnotes",
    "title": "Setting up an AWS environment on Mac",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo view hidden folders on mac press Command + Shift + Dot‚Ü©Ô∏é\nchmod Is a Linux command that sets the permissions for file‚Äôs access. 0400 enables the user to read but can‚Äôt write nor execute.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html",
    "href": "posts/2023_09_10_atom_files/index.html",
    "title": "Extract information from atom files",
    "section": "",
    "text": "Sometimes valuable information often comes in the form of Atom or RSS files, which are essentially plain text formatted in XML. These files serve as a means to distribute content as feeds, ensuring users have access to the latest updates. However, while these files excel at timely information delivery, they can be challenging to decipher and parse. In this blog, I will show how Python can be your key to effortlessly extracting essential insights from these data sources."
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#locate-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#locate-the-data",
    "title": "Extract information from atom files",
    "section": "Locate the data",
    "text": "Locate the data\nFirst, find the data. For this example I will use information regarding public contracts released by the Spanish Ministry for Economic Affairs. These data are published on a daily basis each month. The current month data increases as information for each day is included.\n\nUnderstanding atom files\nAs any other feeds, an atom file is composed of two types of information: metadata and a set of entries.\n\nMetadata provide information about the author of the document (i.e.¬†the Ministry) as well as the URI domain, the date when the information was updated, etc..\nThe entry, on the other hand, provides information about the contract such as the ID of the contract, the price of the contract, the date, etc‚Ä¶\n\n\n\n\nRead the data\nTo read the data we require atoma and feedparser in Python. These will load a &lt;class 'atoma.atom.AtomFeed'&gt; in the system that cab be manipulated. This class has different arguments but the most interesting and the one that contains the relevant information is the &lt;entry&gt;. In our example this class provides information about the different contracts (ID of the contract, price, when the contract was updated, etc‚Ä¶ ) involving public institutions in Spain.\n\n\nCode\nimport atoma\nimport feedparser\n\nfeed = atoma.parse_atom_file('data/licitacionesPerfilesContratanteCompleto3.atom')\n\nprint(type(feed))\n\n\n&lt;class 'atoma.atom.AtomFeed'&gt;\n\n\nThis class contains several entries that are associated with a contract. To understand how many entries are contained in the .atom file we can use the length to show the number of entries in the .atom\n\n\nCode\nprint(len(feed.entries))\n\n\n500"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unknown data Pleasures‚Ä¶",
    "section": "",
    "text": "Extract information from atom files\n\n\n\n\n\n\n\natom\n\n\nrss\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nEdu Gonzalo-Almorox\n\n\n\n\n\n\n  \n\n\n\n\nSetting up an AWS environment on Mac\n\n\n\n\n\n\n\nAWS\n\n\nMLOps\n\n\nEC2\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nEdu Gonzalo-Almorox\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me‚Ä¶",
    "section": "",
    "text": "I am Edu üëã , welcome to my blog!\nI am an economist turned into data scientist with international and multicultural experience in analytics. Throughout my career in several industries, I have been passionate about deriving actionable insights from complex data to address business needs and drive meaningful impact.\nMy areas of expertise involve the application of Machine Learning and (quasi) experimental methods. This blog aims to share reflections about these that may help others in their data journey. In short, a place where I can share different types of digital notes.\nIn addition munging with data, outside work I üö¥‚Äç‚ôÇÔ∏è, sometimes for a while, and üèÉ‚Äç‚ôÇÔ∏è in¬†weird places. I also read and enjoy every kind of good music.\nEnjoy your stay and if you have comments or suggestions please do not hesitate to send me an email to¬†eduardogonzaloalmorox@gmail.com¬†or reach me out on¬†Twitter."
  },
  {
    "objectID": "about.html#in-a-nutshell",
    "href": "about.html#in-a-nutshell",
    "title": "About me‚Ä¶",
    "section": "",
    "text": "I am Edu üëã , welcome to my blog!\nI am an economist turned into data scientist with international and multicultural experience in analytics. Throughout my career in several industries, I have been passionate about deriving actionable insights from complex data to address business needs and drive meaningful impact.\nMy areas of expertise involve the application of Machine Learning and (quasi) experimental methods. This blog aims to share reflections about these that may help others in their data journey. In short, a place where I can share different types of digital notes.\nIn addition munging with data, outside work I üö¥‚Äç‚ôÇÔ∏è, sometimes for a while, and üèÉ‚Äç‚ôÇÔ∏è in¬†weird places. I also read and enjoy every kind of good music.\nEnjoy your stay and if you have comments or suggestions please do not hesitate to send me an email to¬†eduardogonzaloalmorox@gmail.com¬†or reach me out on¬†Twitter."
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#parse-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#parse-the-data",
    "title": "Extract information from atom files",
    "section": "Parse the data",
    "text": "Parse the data\nOnce we have read the data, the next step is to parse the data so we can obtain the information of the contracts in a more structured way. A way to do so consists of querying each element that compose the entry component of the feed. An example of the information provided can be obtained by accessing the summary.value component of the entry.\n\n\nCode\n    print(feed.entries[0].summary.value)\n\n\nId licitaci√≥n: 4270012027200; √ìrgano de Contrataci√≥n: Jefatura de la Secci√≥n Econ√≥mico-Administrativa 27 - Base A√©rea de Getafe; Importe: 50377.14 EUR; Estado: RES\n\n\n\nObtaining information for all entries\nIn the code above, the information obtained corresponds only to the first contract. Yet, as we saw before, there is information referred to 500 public contracts. To parse information for all the entries of the feed object, we can define a function that extracts all the meaningful information for each contract and parses it in a data frame. Then we can loop through all the entries in the file.\nThis part of the code extracts relevant information from the contract and stores it in a dictionary that is saved as a data frame. The last part splits the different elements of the contract\n\nid of the contract\ncontractor or institution that perceives the contract\nprice of the contract\nstatus of the contract\n\n\n\nCode\nimport pandas as pd\n\n\n\ndef get_values(feed, n):  \n    objeto = feed.entries[n].title.value\n    perfil_contratante = feed.entries[n].id_\n    date_update = feed.entries[n].updated\n    link = feed.entries[n].links[0].href\n    licitacion = feed.entries[n].summary.value\n    \n    # Check for missing values and replace them with empty strings\n    objeto = objeto if objeto else \"\"\n    perfil_contratante = perfil_contratante if perfil_contratante else \"\"\n    date_update = date_update if date_update else \"\"\n    link = link if link else \"\"\n    licitacion = licitacion if licitacion else \"\"\n    \n    \n\n    dict = {\n        'objeto': objeto,\n        'perfil_contratante': perfil_contratante,\n        'fecha_actualizacion': date_update,\n        'link': link,\n        'licitacion': licitacion\n    } \n    \n    dict_df = pd.DataFrame(dict,index=[0])\n    \n    return dict_df\n\n\ndef clean_licitacion(df):\n    \n    lic = df[['licitacion']]\n    # Split the 'licitacion' column into new columns: 'id', 'organo', 'importe', 'estado'\n    split_value = lic['licitacion'].str.split(';', expand=True)\n\n\n    lic = df\n    \n    df_new = pd.concat([df, split_value], axis=1)\n   \n    df_new['id_licitacion'] = df_new[0].str.replace('Id licitaci√≥n: ', '')\n    df_new['organo'] = df_new[1].str.replace('√ìrgano de Contrataci√≥n: ', '')\n    df_new['importe'] = df_new[2].str.replace('Importe: ', '')\n    df_new['importe'] = df_new['importe'].str.replace(' EUR', '')\n    df_new['estado'] = df_new[3].str.replace('Estado: ', '')\n  \n     \n    \n    df_new = df_new[['objeto', 'perfil_contratante', 'fecha_actualizacion', 'link', 'id_licitacion', 'organo', 'importe', 'estado']]\n\n    return(df_new)\n\n\n\n\nCode\nlst_df = []\nlst = []\nn_values = list(range(1, len(feed.entries)))\nfor n in n_values:\n            dict_df = get_values(feed, n)\n            lst.append(dict_df)\n\n\nresult_df = pd.concat(lst, ignore_index=True)\n\nfinal_df = clean_licitacion(result_df)\nlst_df.append(final_df)\n\n\n        # Create the master_df by concatenating all the DataFrames\nmaster_df = pd.concat(lst_df, ignore_index=True)\nmaster_df.head(3)\n\n\n\n\n\n\n\n\n\nobjeto\nperfil_contratante\nfecha_actualizacion\nlink\nid_licitacion\norgano\nimporte\nestado\n\n\n\n\n0\nSuministro de energ√≠a el√©ctrica en el Centro d...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:35:38.409000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0109\nDirecci√≥n General del Instituto de Cinematogr...\n59722.28\nRES\n\n\n1\nServicio de duplicado de copias de pel√≠culas, ...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:34:24.393000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0133\nDirecci√≥n General del Instituto de Cinematogr...\n64000\nRES\n\n\n2\nGas Licuado Propano 2013/2014\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:16:16.416000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n204152041512012100\nSecci√≥n de Asuntos Econ√≥micos de la Academia ...\n49586.78\nRES"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#showing-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#showing-the-data",
    "title": "Extract information from atom files",
    "section": "Showing the data",
    "text": "Showing the data\nOnce we have the information parse in a structured format, we can make some analysis and obtain some insights. For instance, we can check what the top institutions by the price of their contracts are.\n\n\nCode\nimport plotly.graph_objs as go\n\n\nmaster_df['importe'] = pd.to_numeric(master_df['importe'], errors='coerce')\ngrouped_data = master_df.groupby('organo')['importe'].sum().reset_index()\n\ngrouped_data = grouped_data.sort_values(by='importe', ascending=False)\ntop_contractors = grouped_data.head(10)\n\nfig = go.Figure(data=[go.Pie(labels=top_contractors['organo'], values=top_contractors['importe'],\n                             hole=0.5, pull=[0.1, 0])])\n\n\n\nfig.update_layout(\n    title=dict(text=\"Top 10 contractors in January 2013\", x = 0.45, y=0.9), \n    legend=dict(orientation=\"h\", y=-5)\n)\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#conclusion",
    "href": "posts/2023_09_10_atom_files/index.html#conclusion",
    "title": "Extract information from atom files",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial you have seen how to parse and represent information from atom files using Python. This analysis can be replicated to the case of rss files.\nI hope you find it useful. Let me know in the comments if you have any questions."
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#atom-files-101",
    "href": "posts/2023_09_10_atom_files/index.html#atom-files-101",
    "title": "Extract information from atom files",
    "section": "Atom files 101",
    "text": "Atom files 101\nFirst, let‚Äôs locate the data .\nIn this blog, we‚Äôll be exploring a valuable source of information: public contracts. In particular, we will use the open data regarding public contracts released by the Spanish Ministry for Economic Affairs. These datasets are regularly updated, with new information becoming available each day throughout the month.\n\nDeconstructing Atom files\nBefore diving into the dasta, it‚Äôs essential to understand the structure of atom files. These files consist of two primary components: metadata and a set of entries.\n\nMetadata provide information about document‚Äôs author (i.e.¬†the Ministry), the domain‚Äôs URI, the date of last update, and more\nOn the other hand, entries contain detailed information about individual contracts, including contract IDs, prices, dates, and much more. These entries are the core of our data exploration journey.\n\n\n\n\nDiving into the data\nTo read the data we require atoma and feedparser in Python. These tool enable us to load a &lt;class 'atoma.atom.AtomFeed'&gt; in the system that can be manipulated. This class has different arguments but the most interesting and the one that contains the relevant information is the &lt;entry&gt;. In our case, this class provides information about the different contracts (ID of the contract, price, when the contract was updated, etc‚Ä¶ ) involving public institutions in Spain.\n\n\nCode\nimport atoma\nimport feedparser\n\nfeed = atoma.parse_atom_file('data/licitacionesPerfilesContratanteCompleto3.atom')\n\nprint(type(feed))\n\n\n&lt;class 'atoma.atom.AtomFeed'&gt;\n\n\nThis class contains several entries that are associated with a contract. To understand how many entries are contained in the .atom file we can use the length attribute to show the number of entries nested in the .atom\n\n\nCode\nprint(len(feed.entries))\n\n\n500"
  },
  {
    "objectID": "posts/2023_09_10_atom_files/index.html#parsing-the-data",
    "href": "posts/2023_09_10_atom_files/index.html#parsing-the-data",
    "title": "Extract information from atom files",
    "section": "Parsing the data",
    "text": "Parsing the data\nOnce we have succesfully read the data, the next step involves parsing the data so we can obtain contract- related information in a more organised and structured manner. Our approach queries each individual element that compose the entry component of the feed. For example, you can get a glimpse of the main elements of the contract by accessing the summary.value component of the entry.\n\n\nCode\n    print(feed.entries[0].summary.value)\n\n\nId licitaci√≥n: 4270012027200; √ìrgano de Contrataci√≥n: Jefatura de la Secci√≥n Econ√≥mico-Administrativa 27 - Base A√©rea de Getafe; Importe: 50377.14 EUR; Estado: RES\n\n\n\nRetrieving Information for all entries\nIn the code snippet provided earlier, we observed how to retrieve information for a single contract. However, as our data source comprises details on a sample of 500 public contracts, we need a more efficient approach. To accomplish this, we can define a function that systematically extracts meaningful information for each contract and organizes it into a structured data frame. We can then iterate through all the entries within the file.\nThe first part of the code (get_values() extracts relevant information from the contract and stores it in a dictionary that is saved as a data frame. The last part splits the different elements of the contract\n\nid of the contract\ncontractor or institution that perceives the contract\nprice of the contract\nstatus of the contract\n\n\n\nCode\nimport pandas as pd\n\n\n\ndef get_values(feed, n):  \n    objeto = feed.entries[n].title.value\n    perfil_contratante = feed.entries[n].id_\n    date_update = feed.entries[n].updated\n    link = feed.entries[n].links[0].href\n    licitacion = feed.entries[n].summary.value\n    \n    # Check for missing values and replace them with empty strings\n    objeto = objeto if objeto else \"\"\n    perfil_contratante = perfil_contratante if perfil_contratante else \"\"\n    date_update = date_update if date_update else \"\"\n    link = link if link else \"\"\n    licitacion = licitacion if licitacion else \"\"\n    \n    \n\n    dict = {\n        'objeto': objeto,\n        'perfil_contratante': perfil_contratante,\n        'fecha_actualizacion': date_update,\n        'link': link,\n        'licitacion': licitacion\n    } \n    \n    dict_df = pd.DataFrame(dict,index=[0])\n    \n    return dict_df\n\n\ndef clean_licitacion(df):\n    \n    lic = df[['licitacion']]\n    # Split the 'licitacion' column into new columns: 'id', 'organo', 'importe', 'estado'\n    split_value = lic['licitacion'].str.split(';', expand=True)\n\n\n    lic = df\n    \n    df_new = pd.concat([df, split_value], axis=1)\n   \n    df_new['id_licitacion'] = df_new[0].str.replace('Id licitaci√≥n: ', '')\n    df_new['organo'] = df_new[1].str.replace('√ìrgano de Contrataci√≥n: ', '')\n    df_new['importe'] = df_new[2].str.replace('Importe: ', '')\n    df_new['importe'] = df_new['importe'].str.replace(' EUR', '')\n    df_new['estado'] = df_new[3].str.replace('Estado: ', '')\n  \n     \n    \n    df_new = df_new[['objeto', 'perfil_contratante', 'fecha_actualizacion', 'link', 'id_licitacion', 'organo', 'importe', 'estado']]\n\n    return(df_new)\n\n\n\n\nCode\nlst_df = []\nlst = []\nn_values = list(range(1, len(feed.entries)))\nfor n in n_values:\n            dict_df = get_values(feed, n)\n            lst.append(dict_df)\n\n\nresult_df = pd.concat(lst, ignore_index=True)\n\nfinal_df = clean_licitacion(result_df)\nlst_df.append(final_df)\n\n\n        # Create the master_df by concatenating all the DataFrames\nmaster_df = pd.concat(lst_df, ignore_index=True)\nmaster_df.head(3)\n\n\n\n\n\n\n\n\n\nobjeto\nperfil_contratante\nfecha_actualizacion\nlink\nid_licitacion\norgano\nimporte\nestado\n\n\n\n\n0\nSuministro de energ√≠a el√©ctrica en el Centro d...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:35:38.409000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0109\nDirecci√≥n General del Instituto de Cinematogr...\n59722.28\nRES\n\n\n1\nServicio de duplicado de copias de pel√≠culas, ...\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:34:24.393000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n2103/0133\nDirecci√≥n General del Instituto de Cinematogr...\n64000\nRES\n\n\n2\nGas Licuado Propano 2013/2014\nhttps://contrataciondelestado.es/sindicacion/l...\n2013-01-11 09:16:16.416000+01:00\nhttps://contrataciondelestado.es/wps/poc?uri=d...\n204152041512012100\nSecci√≥n de Asuntos Econ√≥micos de la Academia ...\n49586.78\nRES"
  }
]